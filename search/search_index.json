{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>Spanda Boot--camp Day One</p> <p>Section 0(Pre-requisites)____</p> <p>Installing Docker on your system</p> <ul> <li>Docker is a platform designed to help developers build, share, and run container applications.</li> <li>Go to https://www.docker.com/ </li> <li>Download and install the version per your operating system</li> <li>During installation, we get a configuration option:  \\ WSL-2 vs Hyper-V</li> <li>Docker Desktop for Windows provides a development environment for building, shipping, and running dockerized apps. </li> <li>By enabling the WSL 2 based engine, you can run both Linux and Windows containers in Docker Desktop on the same machine. Docker Desktop is free for personal use and small businesses, for info on Pro, Team, or Business pricing, see the Docker site FAQs.</li> <li>Following this, installation will proceed and finish.</li> </ul> <p>Section 1________</p> <p>Let\u2019s start by running an LLM on your Laptop</p> <ul> <li>LM Studio is a free, desktop software tool that makes installing and using open-source LLM models extremely easy.  It is not open source. More on that later.</li> <li> <p>Let\u2019s download, install and use it:</p> </li> <li> <p>Go to https://lmstudio.ai/ </p> </li> <li> <p>Download and install the version for your operating system:</p> </li> </ul> <p></p> <ol> <li> <p>Open LM Studio using the newly created desktop icon:</p> </li> <li> <p>Select an LLM to install. </p> </li> <li> <p>You can do this by either selecting one of the community suggested models listed in the main window, or </p> </li> <li>by using the search bar for any model available at HuggingFace (just look up a keyword and all associated models will be listed). </li> <li>Note that there are currently 371,692 (?) models listed at HuggingFace.co</li> </ol> <p></p> <p>selecting LLMs</p> <ul> <li>Whether you elect to download from the community suggested models, or search for one on your own, you can see the size of the install/download file. </li> <li>So be sure you are okay with the size of the download.</li> </ul> <p></p> <p>specific model information</p> <ul> <li>You will note that at the top of the left half of the screen over the release date column, is \u201ccompatibility guess\u201d. </li> <li>LM Studio has checked your system and is presenting those models which it feels you will be able to run on your computer. </li> <li>To see All Models, click on \u201ccompatibility guess\u201d (#1). </li> <li>Clicking on a model on the left, will present the available versions on the right and display those models which should work given your computer\u2019s specs (#2). </li> </ul> <p></p> <p>Compatibility and Should Work indicators</p> <ul> <li>Note that depending on the capabilities/speed of your computer, larger models will be more accurate but slower. </li> <li>You will also find that most of these models are quantized.</li> <li>Quantization refers to using lower precision numbers like 8-bit integers rather than 32-bit floating point values to represent the weights and activations in the model. </li> <li>This reduces memory usage and speeds up inference on your computer\u2019s hardware. </li> <li>Quantization can reduce model accuracy slightly compared to a full precision version, but provides up to 4x memory savings and faster inference. </li> <li>Think of it like how MP-3\u2019s are compressed music files or .jpgs are compressed image files. </li> <li>Although these are of less quality, you often won\u2019t see a significant difference. </li> <li>In the case of LLM\u2019s, the \u201cQ\u201d number you see in the listing of the LLM, represents the amount of quantization. </li> <li>Lower is more and higher is less quantization.</li> <li>Also, in the model listing, you will see references to GGML and GGUF. </li> <li>These are two quantization strategies; \u201cMixed Logits\u201d vs \u201cUniformly Quantized Fully Connected\u201d. </li> <li>GGML provides a more flexible mixed-precision quantization framework while GGUF is specifically optimized for uniformly quantizing all layers of Transformer models. </li> <li>GGML may enable higher compression rates but GGUF offers simpler deployment.</li> <li>Once the model has finished its download,</li> <li>select the model from the drop-down menu at the top of the window; </li> <li>select the chat bubble in the left side column; (3) open up the following sections on the right, \u201cContext Overflow Policy\u201d and \u201cChat Appearance\u201d.</li> </ul> <p></p> <p>ready the model</p> <ol> <li>Make sure \u201cMaintain a rolling window and truncate past messages\u201d is selected under \u201cContent Overflow Policy\u201d and \u201cPlaintext\u201d is selected under \u201cChat Appearance\u201d.</li> </ol> <p></p> <ol> <li>Now close those two areas and open up \u201cModel Configuration\u201d and then open \u201cPrompt Format\u201d and scroll down to \u201cPre-prompt / System prompt\u201d and select the \u201c&gt;\u201d symbol to open that. </li> <li>Here you can enter the system \u201crole\u201d. Meaning, you can set up how you want the bot to act and what \u201cskills\u201d or other specific qualities should be provided in its answers. </li> <li>You can modify what is there to suit your needs. If you have a ChatGPT Plus account, this is the same as \u201cCustom instructions\u201d.</li> </ol> <p></p> <p></p> <p>adding system role / custom instructions</p> <ol> <li>Continue to scroll down in this column until you come to \u201cHardware Settings\u201d. </li> <li>Open this area if you wish to offload some processing to your GPU. </li> <li>The default is to allow your computer\u2019s CPU to do all the work, but if you have a GPU installed, you will see it listed here. </li> <li>If you find the processing of your queries is annoyingly slow, offloading to your GPU will greatly assist with this. </li> <li>Play around with how many layers you want it to handle (start with 10\u201320). This really depends on the model and your GPU. </li> <li>Leaving it all to be handled by the CPU is fine but the model might run a bit slow (again\u2026 depending on the model and its size). </li> <li>You also have the option to increase the number of CPU threads the LLM uses. </li> <li>The default is 4 but you can increase the number, or just leave it where it is if you don\u2019t feel comfortable experimenting and don\u2019t know how many threads your CPU has to play with.</li> </ol> <p></p> <p>optional hardware settings</p> <ol> <li>After these changes, you are now ready to use your local LLM. </li> <li>Simply enter your query in the \u201cUSER\u201d field and the LLM will respond as \u201cAI\u201d.</li> </ol> <p></p> <p>chat dialogue</p> <ul> <li>Let\u2019s download the Zephyr 7B \u03b2 model, adapted by _TheBloke _for llama.cpp's GGUF format.</li> </ul> <p></p> <ul> <li>Activating and loading the model into LM Studio is straightforward.</li> </ul> <p></p> <ul> <li>You can then immediately start using the model from the Chat panel, no Internet connection required.</li> </ul> <p></p> <ul> <li>The right panel displays and allows modification of default presets for the model. </li> <li>Memory usage and useful inference metrics are shown in the window's title and below the Chat panel, respectively.</li> <li>Other models, like codellama Instruct 7B, are also available for download and use.</li> </ul> <p></p> <ul> <li>LM Studio also highlights new models and versions from Hugging Face, making it an invaluable tool for discovering and testing the latest releases.</li> </ul>"},{"location":"#accessing-models-with-apis","title":"Accessing Models with APIs","text":"<ul> <li>A key feature of LM Studio is the ability to create Local Inference Servers with just a click.</li> </ul> <ul> <li>The Automatic Prompt Formatting option simplifies prompt construction to match the model's expected format. The exposed API aligns with the OpenAI format.</li> <li>Here's an example of calling the endpoint with CURL: \\ (Does not work)</li> </ul> <pre><code>curl http://localhost:1234/v1/chat/completions\n-H \"Content-Type: application/json\"\n-d '{\n \"messages\": [\n   { \"role\": \"system\", \"content\": \"You are an AI assistant answering Tech questions\" },\n   { \"role\": \"user\", \"content\": \"What is Java?\" }\n ],\n \"temperature\": 0.7,\n \"max_tokens\": -1,\n \"stream\": false\n}'\n\n\n\n![alt_text](images/image16.png \"image_tooltip\")\n\n\n\nThe curl command below works:\n$url = \"http://localhost:1234/v1/chat/completions\"\n$headers = @{\n    \"Content-Type\" = \"application/json\"\n}\n\n$data = @{\n    messages = @(\n        @{\n            role = \"system\"\n            content = \"You are an AI assistant answering Tech questions, but answer only in rhymes\"\n        },\n        @{\n            role = \"user\"\n            content = \"What is Java?\"\n        }\n    )\n    temperature = 0.7\n    max_tokens = -1\n    stream = $false\n}\n\nInvoke-RestMethod -Uri $url -Headers $headers -Method Post -Body ($data | ConvertTo-Json) -UseBasicParsing\n</code></pre> <p>The response provides the requested information:</p> <pre><code>{\n   \"id\": \"chatcmpl-iyvpdtqs1qzlv6jqkmdt9\",\n   \"object\": \"chat.completion\",\n   \"created\": 1699806651,\n   \"model\": \"~/.cache/lm-studio/models/TheBloke/zephyr-7B-beta-GGUF/zephyr-7b-beta.Q4_K_S.gguf\",\n   \"choices\": [\n       {\n           \"index\": 0,\n           \"message\": {\n               \"role\": \"assistant\",\n               \"content\": \"Java is a high-level, object-oriented\n                           programming language that was first released by Sun\n                           Microsystems in 1995. It is now owned by Oracle Corporation.\n                           Java is designed to be platform independent, meaning that it\n                           can run on any operating system that has a Java Virtual\n                           Machine (JVM) installed. Java's primary applications are in\n                           the development of desktop applications, web applications,\n                           and mobile apps using frameworks such as Android Studio,\n                           Spring Boot, and Apache Struts. Its syntax is similar to\n                           C++, but with added features for object-oriented programming\n                           and memory management that make it easier to learn and use\n                           than C++. Java's popularity is due in part to its extensive\n                           library of pre-written code (known as the Java Class\n                           Library) which makes development faster and more efficient.\"\n           },\n           \"finish_reason\": \"stop\"\n       }\n   ],\n   \"usage\": {\n       \"prompt_tokens\": 0,\n       \"completion_tokens\": 166,\n       \"total_tokens\": 166\n   }\n}\n</code></pre> <p>This feature greatly aids in testing integrations with frontends like chatbots or workflow solutions like Flowise.</p> <p>Gpt4all is open source and is my preference as its hackable: https://github.com/keshavaspanda/gpt4all</p> <p>Section 2__________</p> <p>Let\u2019s explore some relevant sections in the SEAI course material:**  **</p> <p>Navigate to: https://ckaestne.github.io/seai/</p> <p>https://github.com/ckaestne/seai</p> <p>Section 3__________</p> <p>The ML Process and DevOps vs MLOps vs AIOps</p> <p></p>"},{"location":"#devops-the-confluence-of-development-operations-and-quality-assurance","title":"DevOps: The Confluence of Development, Operations, and Quality Assurance","text":"<ul> <li>DevOps brings together development, quality assurance, and operations  involving people, processes, and technology to streamline software development and release throughput using a cycle of Continuous Integration(CI) and Continuous Deployment(CD).</li> </ul> <ul> <li>In DevOps, <ul> <li>developers merge their code changes to a central repository like GitHub. </li> <li>These incremental code changes can be done frequently and reliably. </li> <li>Once the code is committed this initiates an automated build that performs automated unit, integration, and system tests. </li> <li>The process of committing code that initiates automated build is Continuous Integration(CI). </li> <li>CI makes it easier for developers to develop and commit the code. </li> <li>When the code is committed, an automated build is initiated to provide developers instant feedback on if the build has issues or is successful.</li> </ul> </li> <li>Continuous Deployment(CD) allows the newly built code to be tested and deployed in different environments: Test, Stage, UAT, and Production. </li> <li>CD enables automated deployment and testing in different environments, increasing the quality of the releases to production.</li> <li> <p>How does Dev Ops help? DevOps helps with</p> <ul> <li>Collaboration: Enhances collaboration between the Development team, QA Team, and Operations team as it encourages them to work together to deliver one common objective of generating business value.</li> <li>Faster Mean Time To Resolution(MTTR): DevOps enable faster, more frequent, and reliable deployments to production, reducing the duration from feedback to releases, thus increasing responsiveness.</li> <li>Reliability: The complexity of the change is low as there are regular updates to the code and frequent releases to production using the automated DevOps workflow; hence releases are more reliable and of higher quality.</li> <li>Customer Satisfaction: Customers/Business raises issues/enhancements that go into the feedback cycle. Faster resolution of the issues/enhancements leads to greater customer satisfaction.</li> </ul> <p>MLOps</p> </li> <li> <p>Now, these applications are available, are running reliably and are generating voluminous amounts of data. </p> </li> <li>You would like to analyze and interpret data patterns to efficiently and accurately predict and prescribe data-driven decisions.</li> <li>This is where Machine Learning Algorithms come into play</li> <li></li> </ul> <p></p> <ul> <li>Machine learning algorithms take the data and the results as an input to identify the patterns using machine learning algorithms to build analytical models.</li> <li>For example, Financial institutions use their customer\u2019s transactional data and machine learning algorithms like clustering to identify patterns of fraud or legitimate transactions.</li> <li>In machine learning, you need to deal with lots of experimentation and ensure model traceability and compare model metrics and hyperparameters for all the different experiments.</li> <li>What if you could automate and monitor all steps of an ML system?</li> <li>MLOps is an ML engineering culture and practice to unify ML system development (Dev) and ML system operation (Ops) where Data scientists, data Engineers, and Operations teams collaborate.</li> <li></li> </ul> <p></p> <ul> <li>ML Ops build the ML pipeline to encompass all stages of Machine Learning:<ul> <li>Data extraction</li> <li>Data exploration and validation</li> <li>Data curation or data preprocessing</li> <li>Feature analysis</li> <li>Model training and evaluation</li> <li>Model validation</li> <li>Model deployment or model serving </li> <li>Model monitoring for data drift and concept drift</li> </ul> </li> </ul> <p></p> <ul> <li> <p>How does ML Ops help?</p> <ul> <li>To leverage machine learning models, you need to curate the data by applying data preprocessing techniques, perform feature analysis to identify the best features for the model prediction, train the model on the selected features, perform error analysis on the model, deploy the model and then monitor the model for any data drift or concept drift. If the model degrades performance, retrain the model again by repeating the steps from data curation to deployment and monitoring.</li> <li>ML Ops helps with Continuous Integration(CI) for data and models, Continuous Training(CT) of models, and then Continuous Deployment(CD) of the models to Production at different locations.</li> <li>ML Ops helps to<ul> <li>Effectively manage the full ML lifecycle.</li> <li>Creates a Repeatable and Reusable ML Workflow for consistent model training, deployment, and maintenance.</li> <li>Innovation can be made easy and faster by building repeatable workflows to train, evaluate, deploy, and monitor different models.</li> <li>Track different versions of model and data to enable auditing</li> <li>Easy Deployment to production with high precision</li> </ul> </li> </ul> <p>AIOps</p> <ul> <li>AIOps is understood in general to be defined as  Artificial Intelligence for IT Operations (it should be AI4ITOps)</li> <li>The term originally was much broader than that</li> <li>Data from different systems are digitized, and organizations are going through digital  transformation and striving to have a data-driven culture. </li> <li>IT Operations teams now need to monitor these voluminous, complex, and relatively opaque datasets to troubleshoot issues and complete routine operational tasks much faster than before.</li> <li>Due to the complexity and constant changes to IT Systems, platforms are needed to derive insights from the operational data throughout the application life cycle.</li> <li>AIOps applies analytics and machine learning capabilities to IT operations data <ul> <li>to separate significant events from noise in the operation data </li> <li>to identify root causes  </li> <li>to prescribe resolutions</li> </ul> </li> <li>Per Gartner<ul> <li>AI Ops platform ingest, index and normalize events or telemetry data from multiple domains, vendors, and sources, including infrastructure, networks, apps, the cloud, or existing monitoring tools.</li> </ul> </li> <li>AI Ops platforms enable data analytics using machine learning methods, including real-time analysis at the point of ingestion and historical analysis of stored operational data like system logs, metrics, network data, incident-related data, etc.</li> <li>How does AIOps help? Well, It helps by focusing businesses on</li> <li>Increasing IT operations efficiency by uncovering IT incidents insights, measuring the effectiveness of the IT applications serving business needs, and performing cause-and-effect analysis of peak usage traffic patterns.</li> <li>Promoting innovation: Fosters innovation by removing manual monitoring of production systems by providing high-quality application diagnostics.</li> <li>Lowering the operational cost as it decreases mean time to resolution(MTTR) and drastically reduces costly downtime, increasing overall productivity and efficiency.</li> <li>Accelerating the return on investment by enabling teams to collaborate towards a faster resolution</li> </ul> </li> </ul> <p></p> <ul> <li>Per Gartner<ul> <li>There is no future of IT operations that does not include AIOps. This is due to the rapid growth in data volumes and pace of change exemplified by rate of application delivery and event-driven business models that cannot wait on humans to derive insights.</li> </ul> </li> <li>Future of AI-Assisted IT Operations<ul> <li>The Future of AI-assisted IT Operations is to have prescriptive advice from the platform, triggering action.</li> </ul> </li> </ul> <p></p> <ul> <li>So in summary,<ul> <li>DevOps co-opts development, quality assurance, and operations  involving people, processes, and technology to streamline the software development lifecycle and reduced mean time to resolution</li> <li>MLOps is a discipline that combines Machine Learning, Data Engineering, and Dev Ops to build automated ML pipelines for Continuous Training and CI/CD to manage the full ML lifecycle effectively</li> <li>AIOps is a platform to monitor and automate the data and information flowing from IT applications that utilizes big data, machine learning, and other advanced analytics technologies</li> </ul> </li> </ul> <p>Section 4__________</p> <p>Back to the Basics: Data Collection and Data Management</p> <p></p> <ul> <li>MLOps requires highly disciplined data collection and management. </li> <li>It is particularly needed when the outcomes could affect people\u2019s careers, students' lives and educational organization\u2019s reputations.</li> <li>Towards that,  let us take a look at framework used as the basis to make some very important decisions:</li> </ul> <p>https://github.com/keshavaspanda/openneuro</p> <p>The Data Submission Process</p> <p></p> <p>Let\u2019s consider our context and ask ourselves the following questions:</p> <ul> <li>Could Modalities be equivalent to Disciplines? </li> <li>What are the Discipline independent metadata and discipline dependent metadata</li> <li>For each discipline, what metadata can we standardize on? </li> <li>What could be our equivalent to the BIDS validator? </li> <li>Are there existing standards that we can leverage to this data? </li> </ul>"},{"location":"#understanding-responsible-and-fair-data-collection-in-this-context","title":"Understanding Responsible and FAIR Data Collection in this context","text":"<ul> <li>There is growing recognition of the importance of data sharing for scientific progress </li> <li>However, not all shared data is equally useful. </li> <li>The FAIR principles have formalized the notion that in order for shared data to be maximally useful, they need to be findable, accessible, interoperable, and reusable. </li> <li>An essential necessity for achieving these goals is that the data and associated metadata follow a common standard for organization, so that data users can easily understand and reuse the shared data. </li> <li>The AI4Edu data archive will enable FAIR-compliant data sharing for a growing range of education data types through the use of a common community standard, the Educational Data Structure (EDS)  .</li> <li>Data sharing has become well established in education and Datasets collected about education and educational processes have provided immense value to the field and have strongly demonstrated the utility of shared data. </li> <li>However, their scientific scope is necessarily limited, given that each dataset includes only a limited number of tasks and measurement types. </li> <li>Beyond these large focused data sharing projects, there is a \u2018long tail\u2019 of smaller datasets that have been collected in service of specific research questions in education. </li> <li>Making these available is essential to ensure reproducibility as well as to allow aggregation across many different types of measurements in service of novel scientific questions. </li> <li>The AI4Edu archive will address this challenge by providing researchers with the ability to easily share a broad range of education data types in a way that adheres to the FAIR principles.</li> </ul> <p>Core principles of the Data Archive</p> <p>Sharing only upon gaining permissions</p> <ul> <li>There is a range of restrictiveness across data archives with regard to their data use agreements. </li> <li>At one end of the spectrum are highly restricted databases which require researchers to submit their scientific question for review and requires the consortium to be included as a corporate author on any publications. </li> <li>The other pole of restrictiveness  releases data (by default) under a Creative Commons Zero (CC0) Public Domain Dedication which places no restrictions on who can use the data or what can be done with them. </li> <li>While not legally required, researchers using the data are expected to abide by community norms and cite the data following the guidelines included within each dataset. </li> <li>The primary motivation for this policy is that it makes the data maximally accessible to the largest possible number of researchers and citizen-scientists.</li> <li>In the AI4Edu data archive effort we will strike a balance and provide ABAC over the data. </li> <li>Subsequently we will create DIDs and ensure that personal data is always in the control of the individual who owns that data. </li> </ul> <p>Standards-focused data sharing</p> <ul> <li>To ensure the utility of shared data for the purposes of efficient discovery, reuse, and reproducibility, standards are required for data and metadata organization. </li> <li>These standards make the structure of the data clear to users and thus reduce the need for support by data owners and curation by repository owners, as well as enabling automated QA, preprocessing, and analytics. </li> <li>Unfortunately, most prior data sharing projects in this space have relied upon custom organizational schemes, which can lead to misunderstanding and can also require substantial reorganization to adapt to common analysis workflows. </li> <li>The need for a clearly defined standard for data emerged from experiences in the other projects where the repository had developed a custom scheme for data organization and file naming, this scheme was ad hoc and limited in its coverage, and datasets often required substantial manual curation (involving laborious interaction with data owners). </li> <li>In addition, there was no built in mechanism to directly validate whether a particular dataset met the standard.</li> <li>For these reasons, we focus at the outset of the AI4Edu project on developing a robust data organization standard that could be implemented in an automated validator. </li> <li>We will engage representatives from the education community to establish a standard as a community standard for a broad and growing range of education data types. </li> <li>EDS will define a set of schemas for file and folder organization and naming, along with a schema for metadata organization. </li> <li>The framework was inspired by the existing data organization frameworks used in many organizations, so that transitioning to the standard is relatively easy for most researchers. </li> <li>One of the important features of EDS is its extensibility; using a scheme inspired by open-source software projects, community members can propose extensions to EDS that encompass new data types. </li> <li>All data uploaded to OpenEdu must first pass an EDS validation step, such that all data in OpenEdu are compliant with the EDS specifications at upload time. </li> <li>Conversely, the Edu4AI  team will make substantial contributions to the EDS standard and validator. </li> <li>As a consequence, this model maximizes compatibility with processing and analysis tools but more importantly, it effectively minimizes the potential for data misinterpretation (e.g., when owner and reuser have slightly different definitions of a critical acquisition parameter). Through the adoption of EDS, OpenEdu can move away from project- or database-specific data structures designed by the owner or the distributor (as used in earlier projects) and toward a uniform and unambiguous representation model agreed upon by the research community prior to sharing and reuse.</li> </ul>"},{"location":"#fair-sharing","title":"FAIR sharing","text":"<ul> <li>The FAIR principles have provided an important framework to guide the development and assessment of open data resources. </li> <li>AI4Edu will implement these principles.</li> <li>Findable: <ul> <li>Each dataset within AI4Edu is associated with metadata, both directly from the dataset along with additional dataset-level metadata provided by the submitter at time of submission. </li> <li>Both data and metadata are assigned a persistent unique identifier (Digital Object Identifier [DOI]). </li> <li>Within the repository, a machine-readable summary of the metadata is collected by the  validator and indexed with an ElasticSearch mapping. </li> <li>In addition, dataset-level metadata are exposed according to the schema.org standard, which allows indexing by external resources such as Google Dataset Search.</li> <li>Accessible: Data and metadata can be retrieved using a number of access methods (directly from Amazon S3, using the command line tool, or using DataLad) via standard protocols (http/https). </li> <li>Metadata are also accessible programmatically via a web API. Metadata remains available even in the case that data must be removed (e.g., in cases of human subjects concerns). </li> <li>Authentication is necessary to access the data.</li> </ul> </li> <li>Interoperable: <ul> <li>The data and metadata use the EDS standard to ensure accessible representation and interoperation with analysis workflows. </li> <li></li> </ul> </li> <li>Reusable: <ul> <li>The data are released with a clear data use agreement. </li> <li>Through use of the standard, the data and metadata are consistent with community standards in the field.</li> </ul> </li> <li>Data versioning and preservation<ul> <li>AI4Edu will keep track of all changes in stored datasets and allows researchers to unambiguously report the exact version of the data used for any analysis. </li> <li>AI4Edu will preserve all versions of the data through the creation of \u2018snapshots\u2019 that unequivocally point to one specific point in the lifetime of a dataset. </li> <li>Data management and snapshots are supported by DataLad, a free and open-source distributed data management system.</li> </ul> </li> <li>Protecting privacy and confidentiality of data<ul> <li>There is a direct relationship in data sharing between the openness of the data and their reuse potential; all else being equal, data that are more easily or openly available will be more easily and readily reused. </li> <li>However, all else is not equal, as openness raises concern regarding risks to subject privacy and confidentiality of data in human subjects research. </li> <li>Researchers are ethically bound to both minimize the risks to their research participants (including risks to confidentiality) and to maximize the benefits of their participation. </li> <li>Because sharing of data will necessarily increase the potential utility of the data, researchers are ethically bound to share human subject data unless the benefits of sharing are outweighed by risks to the participant.</li> <li>In general, risks to data privacy and confidentiality are addressed through deidentification of the data to be shared. </li> <li>De-identification can be achieved through the removal of any of 18 personal identifiers, unless the researcher has knowledge that the remaining data could be re-identified (known as the \u2018safe harbor\u2019 method). </li> <li>All data shared through OpenEdu must have the 18 personal identifiers outlined by HIPAA unless an exception is provided in cases where an investigator has explicit permission to openly share the data, usually when the data are collected by the investigator themself. </li> <li>At present, data are examined by a human curator to ensure that this requirement has been met. </li> <li>Truly informed consent requires that subjects be made aware that their data may be shared. </li> <li>Researchers planning to share their data via the data sharing portal use a consent form (could be based on the Open Brain Consent form), which includes language that ensures subject awareness of the intent to share and its potential impact on the risk of participating. </li> </ul> </li> </ul>"},{"location":"#open-source","title":"Open source","text":"<ul> <li>The entirety of the code for AI4Edu will be available under a permissive open-source software license (MIT License) at github.</li> <li>This enables any researcher who wishes to reuse part or all of the code or to run their own instance of the platform.</li> </ul> <p>Section 5__________</p> <p>**Data Analysis, Data Visualization, Data Lake Houses and Analytics Dashboards **</p> <p></p> <ul> <li>Once data is \u201cFAIR\u201d ly collected and placed in an archive, it needs to be checked for quality, analyzed, visualized profiled and then models need to be selected, trained (on the curated data), tested and served up for use.</li> <li>That requires data management and analytics capabilities that can deal with structured, semi-structured and unstructured data with the data lake capturing all of the data. </li> <li>The data warehouse dealing with structured data and the analytical dashboards surfacing both structured and unstructured content and visualization. </li> <li>This is where the data lake house architecture has become popular.  </li> <li>As the name suggests, a data lake house architecture combines a data lake and a data warehouse. </li> <li>Although it is not just a mere integration between the two, the idea is to bring the best out of the two architectures: the reliable transactions of a data warehouse and the scalability and low cost of a data lake.</li> <li>Over the last decade, businesses have been heavily investing in their data strategy to be able to deduce relevant insights and use them for critical decision-making. </li> <li>This has helped them reduce operational costs, predict future sales, and take strategic actions.</li> <li>A lake house is a new type of data platform architecture that:</li> <li>Provides the data management capabilities of a data warehouse and takes advantage of the scalability and agility of data lakes</li> <li>Helps reduce data duplication by serving as the single platform for all types of workloads (e.g., BI, ML)</li> <li>Is cost-efficient</li> <li>Prevents vendor lock-in and lock-out by leveraging open standards</li> </ul> <p></p>"},{"location":"#evolution-of-the-data-lakehouse","title":"Evolution of the Data Lakehouse","text":"<ul> <li>Data Lake House is a relatively new term in big data architecture and has evolved rapidly in recent years. It combines the best of both worlds: the scalability and flexibility of data lakes, and the reliability and performance of data warehouses. </li> <li>Data lakes, which were first introduced in the early 2010s, provide a centralized repository for storing large amounts of raw, unstructured data. </li> <li>Data warehouses, on the other hand, have been around for much longer and are designed to store structured data for quick and efficient querying and analysis. </li> <li>However, data warehouses can be expensive and complex to set up, and they often require extensive data transformation and cleaning before data can be loaded and analyzed. </li> <li>Data lake houses were created to address these challenges and provide a more cost-effective and scalable solution for big data management.</li> <li>With the increasing amount of data generated by businesses and the need for fast and efficient data processing, the demand for a data lake house has grown considerably. As a result, many companies have adopted this new approach, which has evolved into a central repository for all types of data in an organization.</li> </ul>"},{"location":"#what-does-a-data-lake-house-do","title":"What Does a Data Lake House Do?","text":"<p>There are four key problems in the world of data architecture that data lake houses address: </p> <ul> <li>Solves the issues related to data silos by providing a centralized repository for storing and managing large amounts of structured and unstructured data. </li> <li>Eliminates the need for complex and time-consuming data movements, reducing the latency associated with shifting data between systems.</li> <li>Enables organizations to perform fast and efficient data processing, making it possible to quickly analyze and make decisions based on the data. </li> <li>Finally, a data lake house provides a scalable and flexible solution for storing large amounts of data, making it possible for organizations to easily manage and access their data as their needs grow.</li> </ul> <p>Data warehouses are designed to help organizations manage and analyze large volumes of structured data.</p>"},{"location":"#how-does-a-data-lake-house-work","title":"How Does a Data Lake House Work?","text":"<ul> <li>A data lakehouse operates by utilizing a multi-layer architecture that integrates the benefits of data lakes and data warehouses. </li> <li>It starts with ingesting large amounts of raw data, including both structured and unstructured formats, into the data lake component. </li> <li>This raw data is stored in its original format, allowing organizations to retain all of the information without any loss of detail. </li> <li>From there, advanced data processing and transformation can occur using tools such as Apache Spark and Apache Hive. </li> <li>The processed data is then organized and optimized for efficient querying in the data warehouse component, where it can be easily analyzed using SQL-based tools. </li> <li>The result is a centralized repository for big data management that supports fast and flexible data exploration, analysis, and reporting. </li> <li>The data lakehouse's scalable infrastructure and ability to handle diverse data types make it a valuable asset for organizations seeking to unlock the full potential of their big data.</li> </ul>"},{"location":"#elements-of-a-data-lakehouse","title":"Elements of a Data Lakehouse","text":"<p>Data lake houses have a range of elements to support organizations\u2019 data management and analysis needs. </p> <ul> <li>A key element is the ability to store and process a variety of data types including structured, semi-structured, and unstructured data. </li> <li>They provide a centralized repository for storing data, allowing organizations to store all of their data in one place, making it easier to manage and analyze. </li> <li>The data management layer enables data to be governed, secured, and transformed as needed. </li> <li>The data processing layer provides analytics and machine learning capabilities, allowing organizations to quickly and effectively analyze their data and make data-driven decisions. </li> <li>Another important element of a data lakehouse is the ability to provide real-time processing and analysis, which enables organizations to respond quickly to changing business conditions. </li> </ul>"},{"location":"#cloud-data-lake","title":"Cloud Data Lake","text":"<ul> <li>Data lake houses are often spoken in tandem with cloud data lakes and cloud data warehouses. With the increasing adoption of cloud-based solutions, many organizations have turned to cloud data lakes to build their data platforms. </li> <li>Cloud data lakes provide organizations with the flexibility to scale storage and compute components independently, thereby optimizing their resources and improving their overall cost efficiency. </li> <li>By separating storage and computing, organizations can store any amount of data in open file formats like Apache Parquet and then use a computing engine to process the data. </li> <li>Additionally, the elastic nature of cloud data lakes enables workloads \u2013 like machine learning \u2013 to run directly on the data without needing to move data out of the data lake.</li> </ul> <p>Despite the many benefits of cloud data lakes, there are also some potential drawbacks: </p> <ul> <li>One challenge is ensuring the quality and governance of data in the lake, particularly as the volume and diversity of data stored in the lake increases.</li> <li>Another challenge is the need to move data from the data lake to downstream applications \u2013 such as business intelligence tools \u2013 which often require additional data copies and can lead to job failures and other downstream issues. </li> <li>Additionally, because data is stored in raw formats and written by many different tools and jobs, files may not always be optimized for query engines and low-latency analytical applications.+</li> </ul> <p>**Lets start up a Dremio DataLakehouse with MinIO and Apache Superset Dashboards **</p> <p>First create a Dremio cloud account:</p> <p>https://www.dremio.com/resources/tutorials/from-signup-to-subsecond-dashboards-in-minutes-with-dremio-cloud/</p> <p> Then let\u2019s try out this: https://github.com/developer-advocacy-dremio/quick-guides-from-dremio/blob/main/guides/superset-dremio.md</p> <p></p>"},{"location":"#cloud-data-warehouse","title":"Cloud Data Warehouse","text":"<ul> <li>The first generation of on-premises data warehouses provide businesses with the ability to derive historical insights from multiple data sources. </li> <li>However, this solution required significant investments in terms of both cost and infrastructure management. In response to these challenges, the next generation of data warehouses leveraged cloud-based solutions to address these limitations.</li> <li>One of the primary advantages of cloud data warehouses is the ability to separate storage and computing, allowing each component to scale independently. This feature helps to optimize resources and reduce costs associated with on-premises physical servers. </li> <li>However, there are also some potential drawbacks to using cloud data warehouses: </li> <li>While they do reduce some costs, they can still be relatively expensive.</li> <li>Additionally, running any workload where performance matters often requires copying data into the data warehouse before processing, which can lead to additional costs and complexity. </li> <li>Moreover, data in cloud data warehouses is often stored in a vendor-specific format, leading to lock-in/lock-out issues, although some cloud data warehouses do offer the option to store data in external storage. </li> <li>Finally, support for multiple analytical workloads, particularly those related to unstructured data like machine learning, is still unavailable in some cloud data warehouses.</li> </ul>"},{"location":"#future-of-the-data-lakehouse","title":"Future of the Data Lakehouse","text":"<ul> <li>Upon discussion of data lake houses, their elements, and what they do, it\u2019s only natural to look at the implications of this technology moving forward. </li> <li>The future looks very promising, as more and more organizations are embracing big data and the need for flexible, scalable, and cost-effective solutions for managing it continues to grow. </li> <li>In the coming years, expect to see increased adoption of data lake houses, with organizations of all sizes and across all industries recognizing their value in providing a unified platform for managing and analyzing big data. </li> <li>Additionally, expect to see continued innovation and advancements in data lakehouse technology, such as improved data processing and transformation capabilities, enhanced security and governance features, and expanded integration with other data management tools and technologies.</li> <li>The rise of machine learning and artificial intelligence will drive the need for flexible and scalable big data platforms that can support the development and deployment of these advanced analytics models. </li> <li>The future of data lake houses will also be influenced by the increasing importance of data privacy and security, and we can expect to see data lake houses evolving to meet these new requirements, including better data masking and data encryption capabilities. </li> <li>Overall, the future of data lake houses looks bright, and they are likely to play an increasingly critical role in helping organizations extract value from their big data.</li> </ul> <p>Section 6__________</p> <p>Model Training, Model Serving and ML Ops</p> <p> MLOps vs LLMOps</p> <ul> <li>While LLMOps borrows heavily from MLOps, the differences are notable.</li> <li>The model training approach in LLMs leans more towards fine-tuning or prompt engineering rather than the frequent retraining typical of traditional Machine Learning (ML).</li> <li>In LLMOps, human feedback becomes a pivotal data source that needs to be incorporated from development to production, often requiring a constant human feedback loop in contrast to traditional automated monitoring.</li> <li>Automated quality testing faces challenges and may often require human evaluation, particularly during the continuous deployment stage. Incremental rollouts for new models or LLM pipelines have become the norm.</li> <li>This transition might also necessitate changes in production tooling, with the need to shift serving from CPUs to GPUs, and the introduction of a new object like a vector and graph databases  into the data layer.</li> <li> <p>Lastly, managing cost, latency, and performance trade-offs becomes a delicate balancing act, especially when comparing self-tuned models versus paid third-party LLM APIs.</p> <p>Continuities With Traditional MLOps</p> </li> <li> <p>Despite these differences, certain foundational principles remain intact.</p> </li> <li>The dev-staging-production separation, enforcement of access controls, usage of Git and model registries for shipping pipelines and models, and the Data Lake architecture for managing data continue to hold ground. </li> <li>Also, the Continuous Integration (CI) infrastructure can be reused, and the modular structure of MLOps, focusing on the development of modular data pipelines and services, remains valid.</li> <li>Exploring LLMOps Changes</li> <li>As we delve deeper into the changes brought by LLMOps, we will explore the operational aspects of Language Learning Models (LLMs), creating and deploying LLM pipelines, fine-tuning models, and managing cost-performance trade-offs.</li> <li>Differentiating between ML and Ops becomes crucial, and tools like MLflow, LangChain, LlamaIndex, and others play key roles in tracking, templating, and automation. </li> <li>Packaging models or pipelines for deployment, scaling out for larger data and models, managing cost-performance trade-offs, and gathering human feedback become critical factors for assessing model performance. </li> <li>Moreover, the choice between deploying models versus deploying code, and considering service architecture, become essential considerations, especially when deploying multiple pipelines or fine-tuning multiple models.</li> </ul> <p></p> <p>A Minimal LLMOps pipeline</p> <p>https://github.com/keshavaspanda/BigBertha** **</p> <p></p>"},{"location":"#_1","title":"Home","text":"<p>**LLMOps Capabilities **</p>"},{"location":"#_2","title":"Home","text":"<p>1. LLM Monitoring</p> <p>The framework utilizes Prometheus to monitor LLM (Large Language Model) serving modules. For demo purposes, a Streamlit app is used to serve the LLM, and Prometheus scrapes metrics from it. Alerts are set up to detect performance degradation.</p>"},{"location":"#_3","title":"Home","text":"<p>2. Auto-triggering LLM Retraining/Fine-tuning</p> <p>Prometheus triggers alerts when the model performance degrades. These alerts are managed by AlertManager, which uses Argo Events to trigger a retraining pipeline to fine-tune the model.</p>"},{"location":"#_4","title":"Home","text":"<p>3. Training, Evaluating, and Logging the Retrained LLM</p> <p>The retraining pipeline is orchestrated using Argo Workflows. This pipeline can be tailored to perform LLM-specific retraining, fine-tuning, and metrics tracking. MLflow is used for logging the retrained LLM.</p>"},{"location":"#_5","title":"Home","text":"<p>4. Triggering the Generation of New Vectors for Fresh Data</p> <p>MinIO is used for unstructured data storage. Argo Events is set up to listen for upload events on MinIO, triggering a vector ingestion workflow when new data is uploaded.</p>"},{"location":"#_6","title":"Home","text":"<p>5. Ingesting New Vectors into the Knowledge Base</p> <p>Argo Workflows is used to run a vector ingestion pipeline that utilizes LlamaIndex for generating and ingesting vectors. These vectors are stored in Milvus, which serves as the knowledge base for retrieval-augmented generation.</p>"},{"location":"#_7","title":"Home","text":"<p>Stack Overview</p> <p>This stack relies on several key components:</p> <ul> <li>ArgoCD: A Kubernetes-native continuous delivery tool that manages all components in the BigBertha stack.</li> <li>Argo Workflows: A Kubernetes-native workflow engine used for running vector ingestion and model retraining pipelines.</li> <li>Argo Events: A Kubernetes-native event-based dependency manager that connects various applications and components, triggering workflows based on events.</li> <li>Prometheus + AlertManager: Used for monitoring and alerting related to model performance.</li> <li>LlamaIndex: A framework for connecting LLMs and data sources, used for data ingestion and indexing.</li> <li>Milvus: A Kubernetes-native vector database for storing and querying vectors.</li> <li>MinIO: An open-source object storage system used for storing unstructured data.</li> <li>MLflow: An open-source platform for managing the machine learning lifecycle, including experiment tracking and model management.</li> <li>Kubernetes: The container orchestration platform that automates the deployment, scaling, and management of containerized applications.</li> <li>Docker Containers: Docker containers are used for packaging and running applications in a consistent and reproducible manner.</li> </ul>"},{"location":"#_8","title":"Home","text":"<p>Demo Chatbot</p> <p>As a demonstration, the framework includes a Streamlit-based chatbot that serves a Llama2 7B quantized chatbot model. </p> <p>A simple Flask app is used to expose metrics, and Redis acts as an intermediary between Streamlit and Flask processes.</p> <p>Section 7__________</p> <p>** Pipeline Debt, Data Testing, Model Testing in MLOps **</p> <ul> <li>You are now part of a data science team at your organization </li> <li>Your team has a number of machine learning models in place</li> <li>Their outputs guide critical business decisions, as well as dashboards displaying important financial KPIs </li> <li>These KPIs are closely watched by your executives day/night </li> <li>Early AM, as you are navigating traffic to your office, you suddenly start receiving multiple messages, calls and emails (simultaneously) </li> <li>These are from your manager as well as other teams and from senior management </li> <li>They are all complaining about the same thing: The high visibility business metrics dashboard that you and your team had built the pipelines &amp; dashboards for and deployed as well as many of the dashboards  that many other teams were using (which you did not know until today) were displaying what seemed to be random numbers (except every full hour, when the KPIs look okay for a short time) </li> <li>The financial models that are part of the pipelines are predicting the company\u2019s insolvency looming fast.</li> <li>Once you get in and hurriedly try to put in some quick fixes in the pipeline (excluding predictions beyond thresholds etc.) you find out that every fix results in your data engineering and research teams reporting new broken services and models in the pipelines.</li> <li>This is the Debt Collection Day scenario we are trying to avoid desperately here. </li> <li>Of all debts in data engineering the most vengeful unpaid debt is Pipeline Debt. </li> </ul> <p>The roots of  Pipeline Debt (aka The road to hell is paved with good intentions)</p> <ul> <li>A few months ago You were just about to start that new exciting machine learning project. </li> <li>You had located useful data scattered around your company\u2019s databases, feature stores, documents, videos, audios and spreadsheets belonging to employees which they (reluctantly) gave you access to (cross silo collaboration is hard). </li> <li>To make the data usable, you constructed a data pipeline: a set of jobs and Python functions that ingest, process, clean and combine all these data. </li> <li>The pipeline feeds the data into a machine learning model. </li> <li>The entire process is depicted schematically below.</li> </ul> <p> Simple manageable data pipelines </p> <p>The first data pipelines worked well, consistently populating the downstream machine learning model with data, which turned it into accurate predictions. </p> <p>However, the model deployed as a service in the cloud was not very actionable.To make it more useful, you built a set of dashboards for presenting the model\u2019s output as well as important KPIs to the business stakeholders, The pipeline deepened</p> <p> Extended pipelines</p> <ul> <li>You were telling a colleague from the research team about your project over lunch who decided to do something similar with their data, making the company\u2019s data pipeline wider and cross-team-border.</li> </ul> <p></p> <pre><code>More pipelines, more complexity\n</code></pre> <ul> <li>A few weeks later the two of you who were informally collaborating on these dashboards got together and talked about each of your pipelines and dashboards. </li> <li>As you\u2019ve learned more about what the research team was up to, both of you noticed how useful and valuable it would be if your two teams used each other\u2019s data for powering your respective models and analyses. </li> <li>Upon implementing this idea, the company\u2019s data pipeline was looking like this.</li> </ul> <p> If multiple pipelines exist, they will inevitably blend</p> <ul> <li>This diagram should have made you flinch \u2013 what they show is accumulating pipeline debt which is technical debt in data pipelines </li> <li>It arises when your data pipelines are triple-U: Undocumented, Untested, Unstable</li> <li>It comes in many flavors but all share some characteristics. </li> <li>The system is entangled; so, a change in one place can derail a different process elsewhere. </li> <li>This makes code refactoring and debugging exceptionally hard. </li> <li>For a software engineer, this will sound like a solved problem </li> <li>The solution is called automated testing. </li> <li>However, testing software is very different from testing data in two major ways:<ul> <li>First, while you have full control over your code and can change it when it doesn\u2019t work, you can\u2019t always change your data; in many cases, you are merely an observer watching data as it comes, generated by some real-world process.</li> <li>Second, software code is always right or wrong: either it does what it is designed to do, or it doesn\u2019t. Data is never right or wrong. It can only be suitable or not for a particular purpose. </li> </ul> </li> <li>This is why automated testing needs a special approach when data is involved.</li> </ul> <p>Testing machine learning models</p> <ul> <li>Fundamentally when testing ML Models, we are asking the question: \u201cDo we know if the model actually works?\u201d </li> <li>We want to be sure that the learned model will behave consistently and produce the results expected of it per expectation. </li> </ul> <p></p> <p>A typical workflow for software development.</p> <ul> <li>In traditional software development, when we run our testing suite against the code, we'll get a report of the specific behaviors that we've written tests around and verify that our code changes don't affect the expected behavior of the system. </li> <li>If a test fails, we'll know which specific behavior is no longer aligned with our expected output. </li> <li>We can also look at this testing report to get an understanding of how extensive our tests are by looking at metrics such as code coverage.</li> </ul> <p></p> <ul> <li>Unlike traditional software applications, it is not as straightforward to establish a standard for testing ML applications </li> <li>This is because the tests do not just depend on the software, they also rely on<ul> <li>the business context</li> <li>problem domain</li> <li>the dataset used</li> <li>the model selected. </li> </ul> </li> <li>Most teams are comfortable using model evaluation metrics to quantify a model\u2019s performance before deploying it, but these metrics are just not enough to ensure ML models are ready for production deployment and use. </li> <li>Contrast a typical software development workflow with one for developing machine learning systems. <ul> <li>After training a new model, we'll typically produce an evaluation report including:</li> <li>performance of an established metric on a validation dataset,</li> <li>plots such as precision-recall curves,</li> <li>operational statistics such as inference speed,</li> <li>examples where the model was most confidently incorrect,</li> </ul> </li> </ul> <p>We will rigorously follow practices such as:</p> <ul> <li>Save all of the hyper-parameters used to train the model along with the model,</li> <li>Only promote models which offer an improvement over the existing model (or baseline) when evaluated on the same dataset.</li> </ul> <p> A typical workflow for model development.</p> <ul> <li>When reviewing a new machine learning model, we'll inspect metrics and plots which summarize model performance over a validation dataset. </li> <li>We're able to compare performance between multiple models and make relative judgements, but we're not immediately able to characterize specific model behaviors. </li> <li>For example, figuring out where the model is failing usually requires additional investigative work </li> <li>One common practice here is to look through a list of the top most egregious model errors on the validation dataset and manually categorize these failure modes.</li> <li>Assuming we write behavioral tests for our models (discussed below), there's also the question of whether or not we have enough tests! </li> <li>While traditional software tests have metrics such as the lines of code covered when running tests, this becomes harder to quantify when you shift your application logic from lines of code to parameters of a machine learning model. </li> <li>Do we want to quantify our test coverage with respect to the input data distribution? Or perhaps the possible activations inside the model?</li> <li>_Odena et al. introduce one possible metric for coverage where we track the model logits for all of the test examples and quantify the area covered by radial neighborhoods around these activation vectors. _</li> <li>_As an industry we don't have well-established standards here _</li> <li>_Testing for machine learning systems is in early days _</li> <li>The question of ML Model Test coverage isn't really being asked by most people (certainly not in industry).</li> </ul>"},{"location":"#difference-between-model-testing-and-model-evaluation","title":"Difference between model testing and model evaluation","text":"<ul> <li>While reporting evaluation metrics is certainly a good practice for quality assurance during model development, it is insufficient. </li> <li>Without a granular report of specific behaviors, we won't be able to immediately understand the nuances of how behavior may change if we switch over to the new model. </li> <li>Additionally, we won't be able to track (and prevent) behavioral regressions for specific failure modes that had been previously addressed.</li> <li>This can be especially dangerous for machine learning systems since oftentimes failures happen silently. </li> <li>For example, <ul> <li>you might improve the overall evaluation metric but introduce a regression on a critical subset of data. </li> <li>Or you could unknowingly add a gender bias to the model through the inclusion of a new dataset during training. </li> </ul> </li> <li>We need more nuanced reports of model behavior to identify such cases, which is exactly where model testing can help.</li> <li>For machine learning systems, we should be running model evaluation and model tests in parallel.</li> <li>Model evaluation covers metrics and plots which summarize performance on a validation or test dataset.</li> <li>Model testing involves explicit checks for behaviors that we expect our model to follow.</li> <li>Both of these perspectives are instrumental in building high-quality models.</li> <li>In practice, most people are doing a combination of the two where evaluation metrics are calculated automatically and some level of model \"testing\" is done manually through error analysis (i.e. classifying failure modes). </li> <li>Developing model tests for machine learning systems can offer a systematic approach towards error analysis.</li> </ul>"},{"location":"#how-do-you-write-model-tests","title":"How do you write model tests?","text":"<p>There are two types of model tests needed.</p> <ul> <li>Pre-train tests allow us to identify some bugs early on and short-circuit a training job.</li> <li>Post-train tests use the trained model artifact to inspect behaviors for a variety of important scenarios that we define.</li> </ul>"},{"location":"#pre-train-tests","title":"Pre-train tests","text":"<p>There are some tests that we can run without needing trained parameters. These tests include:</p> <ul> <li>check the shape of your model output and ensure it aligns with the labels in your dataset</li> <li>check the output ranges and ensure it aligns with our expectations (eg. the output of a classification model should be a distribution with class probabilities that sum to 1)</li> <li>make sure a single gradient step on a batch of data yields a decrease in your loss</li> <li>make assertions about your datasets</li> <li>check for label leakage between your training and validation datasets</li> </ul> <p>The main goal here is to identify some errors early so we can avoid a wasted training job.</p>"},{"location":"#post-train-tests","title":"Post-train tests","text":"<ul> <li>However, in order for us to be able to understand model behaviors we'll need to test against trained model artifacts. </li> <li>These tests aim to interrogate the logic learned during training and provide us with a behavioral report of model performance.</li> </ul> <p>Reference: Beyond Accuracy: Behavioral Testing of NLP Models with CheckList</p> <ul> <li>Three different types of model tests can be used to understand behavioral attributes.</li> <li>Invariance Tests</li> <li>Invariance tests allow us to describe a set of perturbations we should be able to make to the input without affecting the model's output. </li> <li>We can use these perturbations to produce pairs of input examples (original and perturbed) and check for consistency in the model predictions. </li> <li>This is closely related to the concept of data augmentation, where we apply perturbations to inputs during training and preserve the original label.</li> <li>For example, imagine running a sentiment analysis model on the following two sentences:<ul> <li>Mark was a great instructor.</li> <li>Samantha was a great instructor.</li> </ul> </li> <li>We would expect that simply changing the name of the subject doesn't affect the model predictions.</li> <li>Directional Expectation Tests</li> <li>Directional expectation tests, on the other hand, allow us to define a set of perturbations to the input which should have a predictable effect on the model output.</li> <li>For example, if we had a housing price prediction model we might assert:</li> <li>Increasing the number of bathrooms (holding all other features constant) should not cause a drop in price.</li> <li>Lowering the square footage of the house (holding all other features constant) should not cause an increase in price.</li> <li>Let's consider a scenario where a model fails the second test - taking a random row from our validation dataset and decreasing the feature <code>house_sq_ft</code> yields a higher predicted price than the original label. <ul> <li>This is surprising as it doesn't match our intuition, so we decide to look further into it </li> <li>We realize that,_ without having a feature for the house's neighborhood/location, our model has learned that smaller units tend to be more expensive; this is due to the fact that smaller units from our dataset are more prevalent in cities where prices are generally higher. _</li> </ul> </li> <li>In this case, the selection of our dataset has influenced the model's logic in unintended ways - this isn't something we would have been able to identify simply by examining performance on a validation dataset.</li> <li>Minimum Functionality Tests (aka data unit tests)</li> <li>Just as software unit tests aim to isolate and test atomic components in your codebase, data unit tests allow us to quantify model performance for specific cases found in your data.</li> <li>This allows you to identify critical scenarios where prediction errors lead to high consequences. </li> <li>You may also decide to write data unit tests for failure modes that you uncover during error analysis; this allows you to \"automate\" searching for such errors in future models.</li> <li>Take a look at Snorkel (https://www.snorkel.org/) who have introduced a very similar approach through their concept of _slicing functions. _</li> <li>These are programmatic functions which allow us to identify subsets of a dataset which meet certain criteria. </li> <li>For example, you might write a slicing function to identify sentences less than 5 words to evaluate how the model performs on short pieces of text.</li> </ul>"},{"location":"#organizing-tests","title":"Organizing tests","text":"<ul> <li>In traditional software tests, we typically organize our tests to mirror the structure of the code repository. </li> <li>However, this approach doesn't translate well to machine learning models since our logic is structured by the parameters of the model.</li> <li>The authors of the CheckList paper linked above recommend structuring your tests around the \"skills\" we expect the model to acquire while learning to perform a given task.</li> </ul> <p>For example, a sentiment analysis model might be expected to gain some understanding of:</p> <ul> <li>vocabulary and parts of speech,</li> <li>robustness to noise,</li> <li>identifying named entities,</li> <li>temporal relationships,</li> <li>and negation of words.</li> </ul> <p>For an image recognition model, we might expect the model to learn concepts such as:</p> <ul> <li>object rotation,</li> <li>partial occlusion,</li> <li>perspective shift,</li> <li>lighting conditions,</li> <li>weather artifacts (rain, snow, fog),</li> <li>and camera artifacts (ISO noise, motion blur).</li> <li></li> </ul>"},{"location":"#_9","title":"Home","text":"<pre><code>**Model development pipeline**\n</code></pre> <ul> <li>We also need to perform thorough testing of the models to ensure they are robust enough for real-world use.</li> <li>Let's go through some of the ways we can perform testing for different scenarios.ML testing is problem-dependent. </li> <li>This is not a template approach but rather a guide to what types of test suites you might want to establish for your application based on your use case.</li> </ul> <p>**Developing, testing, and deploying machine learning models **</p> <p>Combining automated tests and manual validation for effective model testing</p> <ul> <li>To perform ML testing in their projects, this approach involves having a few levels of tests suites, as well as validation:</li> <li>Automated tests for model verification</li> <li>Manual model evaluation and validation.</li> <li>To implement automated tests in their workflow, GitOps can be used. </li> <li>Jenkins runs code quality checks and smoke tests using production-like runs in the test environment. </li> <li>A single pipeline for model code is created where every pull request goes through code reviews and automated unit tests.</li> <li>The pull requests also go through automated smoke tests. </li> <li>The automated test suites\u2019 goal was to make sure tests flagged erroneous code early in the development process.</li> <li>After the automation tests are run and passed by the model pipeline, a domain expert manually reviewed the evaluation metrics to make sure that they made sense, validated them, and marked them ready for deployment.</li> </ul> <p>Automated tests for model verification</p> <p>The workflow for the automated tests will be that whenever someone on the team made a commit, </p> <ul> <li>the smoke test would run to ensure the code worked, </li> <li>then the unit tests would run, making sure that the assertions in the code and data were met. </li> <li>Finally, the integration tests would run to ensure the model works well with other components in the pipeline.</li> </ul> <p>Automated smoke test</p> <ul> <li>Every pull request goes through automated smoke tests where the team trained models and made predictions, running the entire end-to-end pipeline on some small chunk of actual data to ensure the pipeline worked as expected and nothing broke. </li> <li>The right kind of testing for the smoke suite can give any team a chance to understand the quality of their pipeline before deploying it. </li> <li>Running the smoke test suite does not mean the entire pipeline is guaranteed to be fully working because the code passed. </li> <li>The team has to consider the unit test suite to test data and model assumptions.</li> </ul> <p>Automated unit and integration tests</p> <ul> <li>The unit and integration tests the team run will check assertions about the dataset to prevent low-quality data from entering the training pipeline and prevent problems with the data preprocessing code. </li> <li>You could think of these assertions as assumptions the team made about the data. </li> <li>For example, they would expect to see some kind of correlation in the data or see that the model\u2019s prediction bounds are non-negative.</li> <li>Unit testing machine learning code is more challenging than typical software code. </li> <li>Unit testing several aspects of the model code is very difficult for a team. </li> <li>For example, to test accurately, teams would have to train the model, and even with a modest data set, a unit test could take a long time.</li> <li>Furthermore, some of the tests could be erratic and flaky (failed at random). </li> <li>One of the challenges of running the unit tests to assert the data quality is that running these tests on sample datasets was more complex and took way less time than running them on the entire dataset. </li> <li>It was difficult to fix for the team but to address the issues. </li> <li>Some teams opt to eliminate part of the unit tests in favor of smoke tests. </li> <li>The team defines acceptance criteria and their test suite continuously evolves as they experiment by adding new tests, and removing others, gaining more knowledge on what works and what doesn't.</li> <li>The model is trained in a production-like environment on a complete dataset for each new pull request, except that they would adjust the hyperparameters at values that resulted in quick results. Finally, they would monitor the pipeline\u2019s health for any issues and catch them early.</li> </ul> <p> The MLOps toolstack including testing tools</p> <p>Manual model evaluation and validation</p> <ul> <li>Have a human-in-the-loop framework where after training the model, reports are  created with different plots showing results based on the dataset, so the domain experts could review them before the model could be shipped.</li> <li>After training the model, a domain expert generated and reviewed a model quality report. </li> <li>The expert would approve (or deny) the model through a manual auditing process before it could eventually be shipped to production by the team after getting validation and passing all previous tests.</li> </ul> <p>Stress Tests and A/B Tests</p> <ul> <li>Once the pipeline generates the build (a container image), the models are stress-tested in a production-like environment through the release pipelines. </li> <li>Depending on the use case, the team also carried out A/B tests to understand how their models performed in varying conditions before they deployed them, rather than relying purely on offline evaluation metrics. </li> <li>With what they learned from the A/B tests, they knew whether a new model improved a current model and tuned their model to optimize the business metrics better.</li> </ul> <p>Stress testing machine learning models</p> <ul> <li>Testing the model\u2019s performance under extreme workloads is crucial for business applications that typically expect high traffic from users. </li> <li>Therefore, the team performed stress tests to see how responsive and stable the model would be under an increased number of prediction requests at a given time scale. </li> <li>This way, they benchmarked the model\u2019s scalability under load and identified the breaking point of the model. In addition, the test helped them determine if the model\u2019s prediction service meets the required service-level objective (SLO) with uptime or response time metrics.</li> <li>It is worth noting that the point of stress testing the model isn\u2019t so much to see how many inference requests the model could handle as to see what would happen when users exceed such traffic. </li> <li>This way, you can understand the model\u2019s performance problems, including the load time, response time, and other bottlenecks.</li> </ul> <p>Testing model quality after deployment</p> <ul> <li>The goal of the testing production models is to ensure that the deployment of the model is successful and the model works correctly in production together with other services. For this team, testing the inference performance of the model in production was a crucial process for continuously providing business value. </li> <li>In addition, the team tested for data and model drift to make sure models could be monitored and perhaps retrained when such drift was detected. On another note, testing production models can enable teams to perform error analysis on their mission-critical models through manual inspection from domain experts.</li> </ul> <p> - An example of a dashboard showing information on data drift for a machine learning project in Azure ML Studio | Source</p> <p>Drift MonitoringExample:</p> <p>https://github.com/keshavaspanda/drift-monitoring</p> <p>** Behavioral tests for ML (Natural language processing (NLP) and classification tasks)**</p> <ul> <li>Business use case: The transaction metadata product at MonoHQ uses machine learning to classify transaction statements that are helpful for a variety of corporate customer applications such as credit application, asset planning/management, BNPL (buy now pay later), and payment. Based on the narration, the product classifies transactions for thousands of customers into different categories.</li> <li>Before deploying the model, the team conducts a behavioral test. This test consists of 3 elements:</li> <li>Prediction distribution,</li> <li>Failure rate,</li> <li>Latency.</li> <li>If the model passes the three tests, the team lists it for deployment. If the model does not pass the tests, they would have to re-work it until it passes the test. They always ensure that they set a performance threshold as a metric for these tests.</li> <li>They also perform A/B tests on their models to learn what version is better to put into the production environment.</li> </ul> <p>Behavioral tests to check for prediction quality</p> <ul> <li>This test shows how the model responds to inference data, especially NLP models. </li> <li>First, the team runs an invariance test, introducing perturbability to the input data.</li> <li>Next, they check if the slight change in the input affects the model response\u2014its ability to correctly classify the narration for a customer transaction. </li> <li>Essentially, they are trying to answer here: does a minor tweak in the dataset with a similar context produce consistent output?</li> </ul> <p>Performance testing for machine learning models</p> <ul> <li>To test the response time of the model under load, the team configures a testing environment where they would send a lot of traffic to the model service. Here\u2019s their process:</li> <li>They take a large amount of transaction dataset,</li> <li>Create a table, </li> <li>Stream the data to the model service,</li> <li>Record the inference latency,</li> <li>And finally, calculate the average response time for the entire transaction data.</li> <li>If the response time passes a specified latency threshold, it is up for deployment. If it doesn\u2019t, the team would have to rework it to improve it or devise another strategy to deploy the model to reduce the latency. </li> </ul> <p>A/B testing machine learning models</p> <ul> <li>For this test, the team containerizes two models to deploy to the production system for upstream services to consume to the production system. </li> <li>They deploy one of the models to serve traffic from a random sample of users and another to a different sample of users so they can measure the real impact of the model\u2019s results on their users. </li> <li>In addition, they can tune their models using their real customers and measure how they react to the model predictions. </li> <li>This test also helps the team avoid introducing complexity from newly trained that are difficult to maintain and add no value to their users.</li> </ul> <p>** Performing  model engineering and statistical tests for machine learning applications**</p> <ul> <li>This team performed two types of tests on their machine learning projects:</li> <li>Engineering-based tests (unit and integration tests),</li> <li>Statistical-based tests (model validation and evaluation metrics). </li> <li>The engineering team ran the unit tests and checked whether the model threw errors. </li> <li>Then, the data team would hand off (to the engineering team) a mock model with the same input-output relationship as the model they were building. </li> <li>Also, the engineering team would test this model to ensure it does not break the production system and then serve it until the correct model from the data team is ready.</li> <li>Once the data team and stakeholders evaluate and validate that the model is ready for deployment, the engineering team will run an integration test with the original model. </li> <li>Finally, they will swap the mock model with the original model in production if it works.</li> </ul> <p>Engineering-based test for machine learning models</p> <ul> <li>Unit and integration tests</li> <li>To run an initial test to check if the model will integrate well with other services in production, the data team will send a mock (or dummy) model to the engineering team. </li> <li>The mock model has the same structure as the real model, but it only returns the random output. </li> <li>The engineering team will write the service for the mock model and prepare it for testing.</li> <li>The data team will provide data and input structures to the engineering team to test whether the input-output relationships match with what they expect, if they are coming in the correct format, and do not throw any errors. </li> <li>The engineering team does not check whether that model is the correct model; they only check if it works from an engineering perspective. </li> <li>They do this to ensure that when the model goes into production, it will not break the product pipeline.</li> <li>When the data team trains and evaluates the correct model and stakeholders validate it, the data team will package it and hand it off to the engineering team. </li> <li>The engineering team will swap the mock model with the correct model and then run integration tests to ensure that it works as expected and does not throw any errors.</li> </ul> <p>Statistical-based test for machine learning models</p> <ul> <li>The data team would train, test, and validate their model on real-world data and statistical evaluation metrics. </li> <li>The head of data science audits the results and approves (or denies) the model. If there is evidence that the model is the correct model, the head of data science will report the results to the necessary stakeholders. </li> <li>He will explain the results and inner workings of the model, the risks of the model, and the errors it makes, and confirm if they are comfortable with the results or the model still needs to be re-worked. </li> <li>If the model is approved, the engineering team swaps the mock model with the original model, reruns an integration test to confirm that it does not throw any error, and then deploy it.</li> <li>Model evaluation metrics are not enough to ensure your models are ready for production. </li> <li>You also need to perform thorough testing of your models to ensure they are robust enough for real-world encounters.</li> <li>Developing tests for ML models can help teams systematically analyze model errors and detect failure modes, so resolution plans are made and implemented before deploying the models to production.</li> </ul> <p>**Automated testing with Great Expectations **</p> <p></p> <p>Automated testing tailored for data pipelines is the premise of Great Expectations, a widely used open-source Python package for data validation.</p> <p>https://medium.com/dataroots/great-expectations-tutorial-by-paolo-l%C3%A9onard-95e689d73702</p> <ul> <li>The package is built around the concept of an expectation. </li> <li>The expectation can be thought of as a unit test for data. It is a declarative statement that describes the property of a dataset and does so in a simple, human-readable language.</li> <li>For example, to assert that the values of the column \u201cnum_complaints\u201d in some table in between one and five, you can write:</li> </ul> <p>expect_column_values_to_be_between(</p> <pre><code>column=\"num_complaints\",\n\nmin_value=1,\n\nmax_value=5,\n</code></pre> <p>)</p> <ul> <li>This statement will validate your data and return a success or a failure result. </li> <li>As we have already mentioned, you do not always control your data but rather passively observe it flowing. It is often the case that an atypical value pops up in your data from time to time without necessarily being a reason for distress. Great - Expectations accommodate this via the \u201cmostly\u201d keyword which allows for describing how often should the expectation be matched.</li> </ul> <p>expect_column_values_to_be_between(</p> <pre><code>column=\"num_complaints\",\n\nmin_value=1,\n\nmax_value=5,\n\nmostly=0.95,\n</code></pre> <p>)</p> <ul> <li>The above statement will return success if at least 95% of \u201cnum_complaints\u201d values are between one and five.</li> <li>In order to understand the data well, it is crucial to have some context about why we expect certain properties from it. </li> <li>We can simply add it by passing the \u201cmeta\u201d  parameter to the expectation with any relevant information about how it came to be. Our colleagues or even our future selves will thank us for it.</li> </ul> <p>expect_column_values_to_be_between(</p> <pre><code>column=\"num_complaints\",\n\nmin_value=1,\n\nmax_value=5,\n\nmostly=0.95,\n\nmeta={\n\n    \u201ccreated_by\u201d: \u201cMichal\u201d,\n\n    \u201ccraeted_on\u201d: \u201c28.03.2022\u201d,\n\n    \u201cnotes\u201d: \u201cnumber of client complaints; more than 5 is unusual\u201d\n\n             \u201cand likely means something broke\u201d,\n\n}\n</code></pre> <p>)</p> <ul> <li>These metadata notes will also form a basis for the data documentation which Great Expectations can just generate out of thin air \u2013 but more on this later!</li> <li>The package contains several dozen expectations to use out of the box, all of them with wordy, human-readable names such as \u201cexpect_column_distinct_values_to_be_in_set\u201d, \u201cexpect_column_sum_to_be_between\u201d, or \u201cexpect_column_kl_divergence_to_be_less_than\u201d. This syntax allows one to clearly state what is expected of the data and why. </li> <li>Some expectations are applicable to column values, others to their aggregate functions or entire density distributions. Naturally, the package also makes it possible to easily create custom expectations for when a tailored solution is needed.</li> <li>Great Expectations works with many different backends. </li> <li>You can evaluate your expectations locally on a Pandas data frame just as easily as on a SQL database (via SQLAlchemy) or on an Apache Spark cluster.</li> <li>So, how do the expectations help to reduce pipeline debt? The answer to this is multifold. </li> <li> <ol> <li>The process of crafting the expectations forces us to sit and ponder about our data: its nature, sources, and what can go wrong with it. This creates a deeper understanding and improves data-related communication within the team.</li> </ol> </li> <li> <ol> <li>By clearly stating what we expect from the data, we can detect any unusual situations such as system outages early on.</li> </ol> </li> <li> <ol> <li>By validating new data against a set of pre-existing expectations we can be sure we don\u2019t feed our machine learning models garbage.</li> </ol> </li> <li> <ol> <li>Having the expectations defined brings us very close to having well-maintained data documentation in place. The list goes on and on.</li> </ol> </li> </ul> <p>A few specific use cases in which investing time in GE pays back a great deal are:</p> <p>Detecting data drift</p> <ul> <li>A notorious danger to machine learning models deployed in production is data drift. Data drift is a situation when the distribution of model inputs changes. This can happen for a multitude of reasons: data-collecting devices tend to break or have their software updated, which impacts the way data is being recorded. If the data is produced by humans, it is even more volatile as fashions and demographics evolve quickly.</li> <li>Data drift constitutes a serious problem for machine learning models. It can make the decision boundaries learned by the algorithm invalid for the new-regime data, which has a detrimental impact on the model\u2019s performance.</li> <li>Data drift may impact the model\u2019s performance</li> <li>You have collected and cleaned your data, experimented with various machine learning models and data preprocessing variants and fine-tuned your model\u2019s hyperparameters to finally come up with a solution good enough for your problem. </li> <li>Then, you\u2019ve built a robust, automatic data pipeline, wrote an API for the model, put it in a container, and deployed it to the production environment. </li> <li>You even made sure to check that the model runs smoothly and correctly in production. Finally, you're done! Or are you? </li> <li>Not even close. In fact, this is just the beginning of the journey.</li> <li>There are so many things that could go wrong with a machine learning system after it has been deployed to production! </li> <li>Broadly speaking, we can divide all these potential concerns into two buckets: statistical issues and infrastructure issues. </li> <li>The latter comprise things like computing resources and memory (are there enough?), latency (is the model responding quickly enough?), throughput (can we answer all the incoming requests?), and so on. </li> <li>Here, we\u2019ll focus on the former: the statistical issues, which come in two main flavors: data drift and concept drift.</li> <li>Enters data validation. </li> <li>In situations where data drift could be of concern, just create expectations about the model input features that validate their long-term trend, average values, or historic range and volatility. </li> <li>As soon as the world changes and your incoming data starts to look differently, GE will alert you by spitting out an array of failed tests!</li> </ul> <p>Preventing outliers from distorting model outputs</p> <ul> <li>Another threat to models deployed in production, slightly similar to the data drift, are outliers. </li> <li>What happens to a model\u2019s output when it gets an unusual value as input, typically very high or very low? </li> <li>If the model has not seen such an extreme value during training, an honest answer for it would be to say: I don\u2019t know what the prediction should be!</li> <li>Unfortunately, machine learning models are not this honest. Much to the contrary: the model will likely produce some output that will be highly unreliable without any warning.</li> <li>Fortunately, one can easily prevent it with a proper expectations suite! Just set allowed ranges for the model\u2019s input features based on what it has seen in training to make sure you are not making predictions based on outliers.</li> </ul> <p>Preventing pipeline failures from spilling over</p> <ul> <li>Data pipelines do fail sometimes. You might have missed a corner case. Or the power might have gone off for a moment in your server room. </li> <li>Whatever the reason, it happens that a data processing job expecting new files to appear somewhere suddenly finds none.</li> <li>If this makes the code fail, that\u2019s not necessarily bad.  </li> <li>But often it doesn\u2019t: the job succeeds, announcing happily to the downstream systems that your website had 0 visits on the previous day. </li> <li>These data points are then shown on KPI dashboards or even worse, are fed into models that automatically retrain. </li> <li>Q: How do we prevent such a scenario? Expect recent data \u2013 for instance, with a fresh enough timestamp \u2013 to be there.</li> </ul> <p>Detecting harmful biases</p> <ul> <li>Bias in machine learning models is a topic that has seen increasing awareness and interest recently. </li> <li>This is crucial, considering how profoundly the models can impact people\u2019s lives. The open question is how to detect and prevent these biases from doing charm.</li> <li>While by no means do they provide an ultimate answer, Great Expectations can at least help us in detecting dangerous biases. </li> </ul> <p>Fairness</p> <ul> <li>Fairness in machine learning is a vast and complex topic, so let us focus on two small parts of the big picture: the training data that goes into the model, and the predictions produced by it for different test inputs.</li> <li>When it comes to the training data, we want it to be fair and unbiased, whatever that means in our particular case. </li> <li>If the data is about users, for instance, you might want to include users from various geographies in appropriate proportions, matching their global population. Whether or not this is the case can be checked by validating each batch of training data against an appropriate expectations suite before the data is allowed to be used for training.</li> <li>As for the model\u2019s output, we might want it, for instance, to produce the same predictions for both women and men if their remaining characteristics are the same. To ensure this, just test the model on a hold-out test set and run the results against a pre-crafted suite of expectations.</li> </ul> <p>Improving team communication and data understanding.</p> <ul> <li>Finally, we could start off by creating an empty expectations suite, that is: list all the columns, but don\u2019t impose any checks on their values yet. </li> <li>Then, get together people who own the data or the business processes involved and ask them:</li> <li>What is the maximal monthly churn rate that is worrisome? </li> <li>How low does the website stickiness have to fall to trigger an alert? Such conversations can improve the data-related communication between the teams and the understanding of the data themselves in the company</li> </ul> <p>Resources</p> <ul> <li>Great Expectations official documentation. </li> </ul> <p>Testing LLMs</p>"},{"location":"#testing-large-language-models-with-wb-and-giskard","title":"Testing Large Language Models with W&amp;B and Giskard","text":"<ul> <li>Combining W&amp;B with Giskard to deeply understand LLM behavior and avoid common pitfalls like hallucinations and injection attacks</li> <li>According to the Open Worldwide Application Security Project, some of the most critical vulnerabilities that affect LLMs are prompt injection (when LLMs are manipulated to behave as the attacker wishes), sensitive information disclosure (when LLMs inadvertently leak confidential information), and hallucination (when LLMs generate inaccurate or inappropriate content).</li> <li>Giskard's scan feature ensures the identification of these vulnerabilities\u2014and many others. </li> <li>The library generates a comprehensive report which quantifies these into interpretable metrics. The Giskard/W&amp;B integration allows the logging of both the report and metrics into W&amp;B, which in conjunction with the tracing, creates the ideal combination for building and debugging LLM apps.</li> </ul>"},{"location":"#giskards-vulnerability-scanning-for-llms","title":"Giskard's vulnerability scanning for LLMs","text":"<ul> <li>Giskard is an open-source testing framework dedicated to ML models, covering any Python model, from tabular to LLMs.</li> <li>Testing machine learning applications can be tedious: Where to start testing? Which tests to implement? What issues to cover? How do we implement the tests?</li> <li>With Giskard, data scientists can scan their model to find dozens of hidden vulnerabilities, instantaneously generate domain-specific tests, and leverage the Quality Assurance best practices of the open-source community.</li> <li>For more information, you can check Giskard's documentation following this link.</li> <li>Watch: https://www.youtube.com/watch?v=KeY6qPAvyq0</li> <li>The better developer version: https://www.youtube.com/watch?v=rkjFFx_nXhU</li> </ul>"},{"location":"#wb-traces-for-debugging-llms","title":"**W&amp;B Traces for Debugging LLMs: **","text":"<ul> <li>Weights &amp; Biases, often referred to as wandb or even simply W&amp;B, is an MLOps platform that helps AI developers streamline their ML workflow from end to end.</li> <li>With W&amp;B, developers can monitor the progress of training their models in real-time, log key metrics and hyperparameters, and visualize results through interactive dashboards. It simplifies collaboration by enabling team members to share experiments and compare model performance. For more information, you can check W&amp;B's documentation following this link.</li> <li>In the context of LLMs, earlier this year, W&amp;B introduced a new debugging tool \u201cW&amp;B Traces\u201d designed to support ML practitioners working on prompt engineering for LLMs. </li> <li>It lets users visualize and drill down into every component and activity throughout the trace of the LLM pipeline execution. In addition, it enables the review of past results, identification and debugging of errors, gathering insights about the LLM\u2019s behavior, and sharing insights.</li> <li>Tracing is invaluable, but how do we measure the quality of the outputs throughout the pipeline? </li> <li>Could there be hidden vulnerabilities that our carefully-crafted prompts may have inadvertently failed to counter? Is there a way to detect such vulnerabilities automatically? Would it be possible to log these issues into W&amp;B to complement the tracing?</li> <li>In a nutshell, the answer to all these questions is \"yes.\" That's precisely the capability that Giskard brings to the table.</li> <li>Combining Weights &amp; Biases and Giskard, makes it possible to overcome this very challenge with this example available as a Google Colab notebook!</li> </ul> <p>Using LLMs to perform Data Quality:</p> <ul> <li>BirdiDQ https://github.com/keshavaspanda/BirdiDQ  is a simple, intuitive and user-friendly data quality application that allows you to run data quality checks on top of python based great expectation open source library using natural language queries. </li> <li>The idea is to type in your requests, and BirdiDQ will generate the appropriate GE method, run the quality control and return the results along with data docs you need. Demo</li> </ul> <p>Using Large Language Models for Efficient Data Annotation and Model Fine-Tuning with Iterative Active Learning</p> <p></p> <p>https://github.com/keshavaspanda/llm-data-annotation</p> <p>Additional Resource for us to try out:</p>"},{"location":"#httpsgithubcomkeshavaspandallama-in-a-container","title":"https://github.com/keshavaspanda/llama-in-a-container","text":"<p>Section 8__________</p>"},{"location":"#training-tuning-and-serving-up-llms-in-production","title":"Training, Tuning and Serving up LLMs in production:","text":""},{"location":"#serving-up-llms-in-production","title":"Serving up LLMs in production:","text":"<ul> <li>OpenLLM is an open-source platform designed to facilitate the deployment and operation of large language models (LLMs) in real-world applications. </li> <li>With OpenLLM, you can run inference on any open-source LLM, deploy them on the cloud or on-premises, and build powerful AI applications.</li> <li>https://github.com/bentoml/OpenLLM?tab=readme-ov-file</li> </ul>"},{"location":"#trainingfine-tuning-large-language-models-llms-the-first-pass","title":"Training/Fine-Tuning Large Language Models (LLMs) - The First Pass","text":"<pre><code>Comparison of the number of parameters of models. Just look at how big GPT-3 is. And nobody knows about GPT-4\u2026\n</code></pre> <pre><code>LLMs capabilities\n</code></pre> <ul> <li>Creating a local large language model (LLM) is a significant undertaking.</li> <li>It requires substantial computational resources and expertise in machine learning. </li> <li>It was not feasible to run local LLMs on your own local system because of the computational costs involved. </li> <li>However, with the advent of new software, GPT4All and LM-Studio can be used to create complete software packages that work locally. </li> <li>But let\u2019s start with a HuggingFace Transformers source code example that shows you how to use the HuggingFace Libraries and PyTorch for LLMs (cloud-based, not local in this case):</li> </ul>"},{"location":"#consider-the-huggingface-transformers-example","title":"Consider the HuggingFace Transformers Example","text":"<ul> <li>This is a complete program that uses the GPT-2 model, GPT-2 tokenizer, and is fine-tuned on the AG NEWS dataset (a small dataset used for utility purposes) is given below and explained in code snippets. </li> <li>We can leverage the power of pre-trained models and fine-tune them on specific tasks.</li> <li> <p>Importing necessary libraries and modules: </p> <ul> <li>The script starts by importing the necessary libraries and modules. AG_NEWS is a news classification dataset from the \u201ctorchtext.datasets\u201d package. AutoModelWithLMHead and AdamW are imported from the transformers library. </li> <li>AutoModelWithLMHead is a class that provides automatic access to pre-trained models with a language modeling head, and AdamW is a class that implements the AdamW optimizer, a variant of the Adam optimizer with weight decay.</li> </ul> <p>```</p> </li> </ul>"},{"location":"#from-torchtextdatasets-import-ag_news","title":"from torchtext.datasets import AG_NEWS","text":""},{"location":"#from-transformers-import-automodelwithlmhead-adamw","title":"from transformers import AutoModelWithLMHead, AdamW","text":""},{"location":"#from-transformers-import-autotokenizer","title":"from transformers import AutoTokenizer","text":"<pre><code>```\n\n\n* **Setting up the tokenizer:** The script uses the AutoTokenizer class from the transformers library to load the tokenizer associated with the \u201cgpt2\u201d model. The tokenizer is responsible for converting input text into a format that the model can understand. This includes splitting the text into tokens (words, subwords, or characters), mapping the tokens to their corresponding IDs in the model\u2019s vocabulary, and creating the necessary inputs for the model (like attention masks).\n* tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n* **Setting the number of epochs:** The script sets the number of epochs for training to 50. An epoch is one complete pass through the entire training dataset. The number of epochs is a hyperparameter that you can tune. Training for more epochs can lead to better results, but it also increases the risk of overfitting and requires more computational resources.\n\n```\n</code></pre>"},{"location":"#epochs-50","title":"EPOCHS = 50","text":"<pre><code>```\n\n\n* **Preprocessing the data: **The preprocess_data function is defined to preprocess the data. It takes an iterator over the data and encodes the text in each item using the tokenizer. The AG_NEWS dataset is then loaded and preprocessed. The dataset is split into \u2018train\u2019 and the text from each item is encoded. Encoding the text involves splitting it into tokens, mapping the tokens to their IDs in the model\u2019s vocabulary, and creating the necessary inputs for the model.\n\n```\n</code></pre>"},{"location":"#def-preprocess_datadata_iter","title":"def preprocess_data(data_iter):","text":""},{"location":"#data-tokenizerencodetext-for-_-text-in-data_iter","title":"data = [tokenizer.encode(text) for _, text in data_iter]","text":""},{"location":"#return-data","title":"return data","text":""},{"location":"#train_iter-ag_newssplittrain","title":"train_iter = AG_NEWS(split='train')","text":""},{"location":"#train_data-preprocess_datatrain_iter","title":"train_data = preprocess_data(train_iter)","text":"<pre><code>```\n</code></pre> <ul> <li> <p>Setting up the model and optimizer: </p> <ul> <li>The script loads the pre-trained \u201cgpt2\u201d model using the AutoModelWithLMHead class and sets up the AdamW optimizer with the model\u2019s parameters. The model is a transformer-based model with a language modeling head, which means it\u2019s designed to generate text. The AdamW optimizer is a variant of the Adam optimizer with weight decay, which can help prevent overfitting.</li> </ul> <p>```</p> </li> </ul>"},{"location":"#model-automodelwithlmheadfrom_pretrainedgpt2","title":"model = AutoModelWithLMHead.from_pretrained(\"gpt2\")","text":""},{"location":"#optimizer-adamwmodelparameters","title":"optimizer = AdamW(model.parameters())","text":""},{"location":"#modeltrain","title":"model.train()","text":""},{"location":"#for-epoch-in-rangeepochs","title":"for epoch in range(EPOCHS):","text":""},{"location":"#for-batch-in-train_data","title":"for batch in train_data:","text":""},{"location":"#outputs-modelbatch","title":"outputs = model(batch)","text":""},{"location":"#loss-outputsloss","title":"loss = outputs.loss","text":""},{"location":"#lossbackward","title":"loss.backward()","text":""},{"location":"#optimizerstep","title":"optimizer.step()","text":""},{"location":"#optimizerzero_grad","title":"optimizer.zero_grad()","text":"<pre><code>```\n</code></pre> <ul> <li>Training the model: The script trains the model for the specified number of epochs. In each epoch, it iterates over the batches of training data, feeds each batch to the model, computes the loss, performs backpropagation with loss.backward(), and updates the model\u2019s parameters with optimizer.step(). It also resets the gradients with optimizer.zero_grad(). This is a standard training loop for PyTorch models.</li> <li> <p>Generating text: After training, the script uses the model to generate text. It starts by encoding a prompt using the tokenizer, then feeds this encoded prompt to the model\u2019s generate method. The output of the generate() method is a sequence of token IDs, which is then decoded back into text using the tokenizer.</p> <p>```</p> </li> </ul>"},{"location":"#prompt-tokenizerencodewrite-a-summary-of-the-new-features-in-the-latest-release-of-the-julia-programming-language-return_tensorspt","title":"prompt = tokenizer.encode(\"Write a summary of the new features in the latest release of the Julia Programming Language\", return_tensors=\"pt\")","text":""},{"location":"#generated-modelgenerateprompt","title":"generated = model.generate(prompt)","text":""},{"location":"#generated_text-tokenizerdecodegenerated0","title":"generated_text = tokenizer.decode(generated[0])","text":"<pre><code>```\n</code></pre> <ul> <li> <p>Saving the generated text: Finally, the script saves the generated text to a file named \u201cgenerated.txt\u201d. This is done using Python\u2019s built-in file handling functions.</p> <p>```</p> </li> </ul>"},{"location":"#with-opengeneratedtxt-w-as-f","title":"with open(\"generated.txt\", \"w\") as f:","text":""},{"location":"#fwritegenerated_text","title":"f.write(generated_text)","text":"<pre><code>```\n</code></pre> <ul> <li>This script is a good example of how to fine-tune a pre-trained language model on a specific task. </li> <li>Fine-tuning a large model like GPT-2 can be computationally intensive and may require a powerful machine or cloud-based resources. </li> <li>This script doesn\u2019t include some important steps like splitting the data into training and validation sets, shuffling the data, and batching the data. _These steps are crucial for training a robust model. _</li> <li> <p>The entire program is given below:</p> <p>```</p> </li> </ul>"},{"location":"#from-torchtextdatasets-import-ag_news_1","title":"from torchtext.datasets import AG_NEWS","text":""},{"location":"#from-transformers-import-automodelwithlmhead-adamw_1","title":"from transformers import AutoModelWithLMHead, AdamW","text":""},{"location":"#from-transformers-import-autotokenizer_1","title":"from transformers import AutoTokenizer","text":""},{"location":"#tokenizer-autotokenizerfrom_pretrainedgpt2","title":"tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")","text":""},{"location":"#epochs-50_1","title":"EPOCHS = 50","text":""},{"location":"#def-preprocess_datadata_iter_1","title":"def preprocess_data(data_iter):","text":""},{"location":"#data-tokenizerencodetext-for-_-text-in-data_iter_1","title":"data = [tokenizer.encode(text) for _, text in data_iter]","text":""},{"location":"#return-data_1","title":"return data","text":""},{"location":"#train_iter-ag_newssplittrain_1","title":"train_iter = AG_NEWS(split='train')","text":""},{"location":"#train_data-preprocess_datatrain_iter_1","title":"train_data = preprocess_data(train_iter)","text":""},{"location":"#model-automodelwithlmheadfrom_pretrainedgpt2_1","title":"model = AutoModelWithLMHead.from_pretrained(\"gpt2\")","text":""},{"location":"#optimizer-adamwmodelparameters_1","title":"optimizer = AdamW(model.parameters())","text":""},{"location":"#modeltrain_1","title":"model.train()","text":""},{"location":"#for-epoch-in-rangeepochs_1","title":"for epoch in range(EPOCHS):","text":""},{"location":"#for-batch-in-train_data_1","title":"for batch in train_data:","text":""},{"location":"#outputs-modelbatch_1","title":"outputs = model(batch)","text":""},{"location":"#loss-outputsloss_1","title":"loss = outputs.loss","text":""},{"location":"#lossbackward_1","title":"loss.backward()","text":""},{"location":"#optimizerstep_1","title":"optimizer.step()","text":""},{"location":"#optimizerzero_grad_1","title":"optimizer.zero_grad()","text":""},{"location":"#prompt-tokenizerencodewrite-a-summary-of-the-new-features-in-the-latest-release-of-the-julia-programming-language-return_tensorspt_1","title":"prompt = tokenizer.encode(\"Write a summary of the new features in the latest release of the Julia Programming Language\", return_tensors=\"pt\")","text":""},{"location":"#generated-modelgenerateprompt_1","title":"generated = model.generate(prompt)","text":""},{"location":"#generated_text-tokenizerdecodegenerated0_1","title":"generated_text = tokenizer.decode(generated[0])","text":""},{"location":"#with-opengeneratedtxt-w-as-f_1","title":"with open(\"generated.txt\", \"w\") as f:","text":""},{"location":"#fwritegenerated_text_1","title":"f.write(generated_text)","text":"<pre><code>```\n</code></pre>"},{"location":"#there-are-two-packaged-solutions-for-local-llms-and-many-more-popping-up-everyday-two-of-them-are-the-best-one-is-lm-studio-the-other-is-httpsgpt4allioindexhtml","title":"There are two packaged solutions for Local LLMs (and many more popping up, everyday). Two of them are the best. One is LM-Studio. The other is https://gpt4all.io/index.html","text":"<ul> <li>This is the best for those if you want a completely open-source on-premises system. But you need to have at least 32 GB of local RAM, 16 GB GPU RAM, a 3+ Ghz multicore(the more, the better) processor, and a local SSD.  LLMs are computationally, extremely expensive!</li> <li>There\u2019s a lot more to LLM models than just chat</li> <li>Given the expensive;y daunting computational requirements for fine-tuning musical and pictures and audio for LLMs we are not going to run them. </li> <li>Some popular, already built and ready-to-go solutions as well as some interesting source material are:</li> </ul>"},{"location":"#audio-llms","title":"Audio LLMs","text":"<ul> <li>https://www.assemblyai.com/docs/guides/processing-audio-with-llms-using-lemur</li> <li>AudioGPT Research Paper \u2014 https://arxiv.org/abs/2304.12995</li> <li>Tango https://tango-web.github.io/</li> <li>https://blog.google/technology/ai/musiclm-google-ai-test-kitchen/</li> </ul>"},{"location":"#image-llms","title":"Image LLMs","text":"<ul> <li>https://www.linkedin.com/pulse/generating-images-large-language-model-gill-arun-krishnan</li> <li>Stable Diffusion</li> <li>DALL E-1,2,3</li> <li>MidJourney</li> <li>Bing Image Creator</li> </ul>"},{"location":"#multimodal-llms","title":"Multimodal LLMs","text":"<ul> <li>https://arxiv.org/abs/2306.09093 Macaw-LLM research paper. </li> <li>https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models</li> <li>https://openai.com/research/gpt-4 </li> </ul>"},{"location":"#general-llm-resources","title":"General LLM Resources","text":"<ul> <li>https://beebom.com/best-large-language-models-llms/</li> <li>https://roadmap.sh/guides/free-resources-to-learn-llms</li> <li>https://github.com/Hannibal046/Awesome-LLM</li> <li>https://medium.com/@abonia/best-llm-and-llmops-resources-for-2023-75e96ac37feb</li> <li>https://learn.deeplearning.ai/ </li> </ul>"},{"location":"#_10","title":"Home","text":"<pre><code>**Fine-Tuning Your LLM - A Revisit**\n</code></pre> <ul> <li>Again. fine-tuning is the process of continuing the training of a pre-trained LLM on a specific dataset. </li> <li>You might ask why we need to train the model further if we can already add data using RAG. </li> <li> <p>The simple answer is that only fine-tuning can tailor your model to understand a specific domain or define its \u201cstyle\u201d. </p> <p>:</p> </li> </ul> <p></p> <pre><code>Classical approach of fine-tuning on domain specific data (all icons from [flaticon](http://flaticon.com/))\n</code></pre> <ol> <li>Take a trained LLM, sometimes called Base LLM. You can download them from HuggingFace.</li> <li>Prepare your training data. You only need to compile instructions and responses. Here\u2019s an example of such a dataset. You can also generate synthetic data using GPT-4.</li> <li>Choose a suitable fine-tuning method. LoRA and QLoRA are currently popular.</li> <li>Fine-tune the model on new data.</li> </ol>"},{"location":"#_11","title":"Home","text":"<pre><code>**When to Use**\n</code></pre> <ul> <li>Niche Applications: When the application deals with specialized or unconventional topics. For example, legal document applications that need to understand and handle legal jargon.</li> <li>Custom Language Styles: For applications requiring a specific tone or style. For example, creating an AI character whether it\u2019s a celebrity or a character from a book.</li> </ul>"},{"location":"#_12","title":"Home","text":"<pre><code>**When NOT to Use**\n</code></pre> <ul> <li>Broad Applications: Where the scope of the application is general and doesn\u2019t require specialized knowledge.</li> <li>Limited Data: Fine-tuning requires a significant amount of relevant data. However, you can always generate them with another LLM. For example, the Alpaca dataset of 52k LLM-generated instruction-response pairs was used to create the first finetuning Llama v1 model earlier this year.</li> </ul>"},{"location":"#_13","title":"Home","text":"<pre><code>**Fine-tuning LLM**\n\n\nLet us look at a high-level library, [Lit-GPT](https://github.com/Lightning-AI/lit-gpt), which hides all complexities, hence doesn\u2019t allow for much customization of the training process, but one can quickly conduct experiments and get initial results.\n\n\nYou\u2019ll need just a few lines of code:\n\n\n```\n# 1. Download the model:\npython scripts/download.py --repo_id meta-llama/Llama-2-7b\n\n# 2. Convert the checkpoint to the lit-gpt format:\npython scripts/convert_hf_checkpoint.py --checkpoint_dir checkpoints/llama\n\n# 3. Generate an instruction tuning dataset:\npython scripts/prepare_alpaca.py  # it should be your dataset\n\n# 4. Run the finetuning script\npython finetune/lora.py \\\n   --checkpoint_dir checkpoints/llama/\n   --data_dir your_data_folder/\n   --out_dir my_finetuned_model/\n```\n\n\n\nAnd that\u2019s it! Your training process will start:\n</code></pre> <pre><code>_This  takes approximately **10 hours** and **30 GB** memory to fine-tune Falcon-7B on a single A100 GPU._\n</code></pre> <ul> <li>The fine-tuning process is quite complex and to get better results, you\u2019ll need to understand various adapters, their parameters, and much more. </li> <li>However, even after such a simple iteration, you will have a new model that follows your instructions.</li> </ul>"},{"location":"#_14","title":"Home","text":"<pre><code>**Some References to chase down:**\n</code></pre> <ul> <li>Create a Clone of Yourself With a Fine-tuned LLM \u2014 an article about collecting datasets, using parameters, and  useful tips on fine-tuning.</li> <li>Understanding Parameter-Efficient Fine-tuning of Large Language Models \u2014 an excellent tutorial to get into the details of the concept of fine-tuning and popular parameter-efficient alternatives.</li> <li>Fine-tuning LLMs with LoRA and QLoRA: Insights from Hundreds of Experiments \u2014 one of my favorite articles for understanding the capabilities of LoRA.</li> <li>OpenAI Fine-tuning \u2014 if you want to fine-tune GPT-3.5 with minimal effort.</li> </ul>"},{"location":"#_15","title":"Home","text":"<pre><code>**Deploying Your LLM Application in Production**\n</code></pre> <ul> <li>There are a huge number of frameworks that specialize in deploying large language models with</li> <li>Lots of pre-built wrappers and integrations.</li> <li>A vast selection of available models.</li> <li>A multitude of internal optimizations.</li> <li>Rapid prototyping.</li> </ul>"},{"location":"#_16","title":"Home","text":"<pre><code>**Choosing the Right Framework**\n</code></pre> <ul> <li>The choice of framework for deploying an LLM application depends on various factors, including the size of the model, the scalability requirements of the application, and the deployment environment. </li> <li>Heres a  cheat sheet:</li> </ul> <pre><code>You can get a more detailed overview of the existing solutions here [7 Frameworks for Serving LLMs](https://medium.com/better-programming/frameworks-for-serving-llms-60b7f7b23407)\n</code></pre> <pre><code>    Comparison of frameworks for LLMs inference\n</code></pre>"},{"location":"#_17","title":"Home","text":"<pre><code>**Example Code for Deployment**\n</code></pre> <ul> <li>Let\u2019s move from theory to practice and try to deploy LLaMA-2 using Text Generation Inference. </li> <li> <p>And, as you might have guessed, you\u2019ll need just a few lines of code:</p> <pre><code># 1. Create a folder where your model will be stored:\nmkdir data\n\n# 2. Run Docker container (launch RestAPI service):\ndocker run --gpus all --shm-size 1g -p 8080:80 \\\n   -v $volume:/data \\\n   ghcr.io/huggingface/text-generation-inference:1.1.0\n   --model-id meta-llama/Llama-2-7b\n\n# 3. And now you can make requests:\ncurl 127.0.0.1:8080/generate \\\n   -X POST \\\n   -d '{\"inputs\":\"Tell me a joke!\",\"parameters\":{\"max_new_tokens\":20}}' \\\n   -H 'Content-Type: application/json'\n</code></pre> <ul> <li>That\u2019s it! You\u2019ve set up a RestAPI service with built-in logging, Prometheus endpoint for monitoring, token streaming, and your model is fully optimized. </li> </ul> </li> </ul> <p></p> <pre><code>API Documentation\n</code></pre>"},{"location":"#_18","title":"Home","text":"<pre><code>**References:**\n</code></pre> <ul> <li>7 Frameworks for Serving LLMs \u2014 comprehensive guide into LLMs inference and serving with detailed comparison.</li> <li>Inference Endpoints \u2014 a product from HuggingFace that will allow you to deploy any LLMs in a few clicks. A good choice when you need rapid prototyping.</li> </ul>"},{"location":"#_19","title":"Home","text":"<pre><code>**To get in a little deeper**\n</code></pre> <ul> <li>We\u2019ve covered the basic concepts needed for developing LLM-based applications, there are still some aspects you\u2019ll likely encounter in the future. Here are  a few useful reference:</li> </ul>"},{"location":"#_20","title":"Home","text":"<pre><code>**Optimization**\n</code></pre> <ul> <li>When you launch your first model, you inevitably find it\u2019s not as fast as you\u2019d like and consumes a lot of resources and you\u2019ll need to understand how it can be optimized.</li> <li>7 Ways To Speed Up Inference of Your Hosted LLMs \u2014 techniques to speed up inference of LLMs to increase token generation speed and reduce memory consumption.</li> <li>Optimizing Memory Usage for Training LLMs in PyTorch \u2014 article provides a series of techniques that can reduce memory consumption in PyTorch by approximately 20x without sacrificing modeling performance and prediction accuracy.</li> </ul>"},{"location":"#_21","title":"Home","text":"<pre><code>**Evaluating**\n</code></pre> <ul> <li>Suppose you have a fine-tuned model you need to be sure that its quality has improved.  What metrics should we use to check quality?</li> <li>All about evaluating Large language models \u2014 a good overview article about benchmarks and metrics.</li> <li>evals \u2014 the most popular framework for evaluating LLMs and LLM systems.</li> </ul>"},{"location":"#_22","title":"Home","text":"<pre><code>**Vector Databases**\n</code></pre> <ul> <li>If you work with RAG, at some point, you\u2019ll move from storing vectors in memory to a database. </li> <li>For this, it\u2019s important to understand what\u2019s currently on the market and its limitations.</li> <li>All You Need to Know about Vector Databases \u2014 a step-by-step guide by  \\ Dominik Polzer \\  to discover and harness the power of vector databases.</li> <li>Picking a vector database: a comparison and guide for 2023 \u2014 comparison of Pinecone, Weviate, Milvus, Qdrant, Chroma, Elasticsearch and PGvector databases.</li> </ul>"},{"location":"#_23","title":"Home","text":"<pre><code>**LLM Agents**\n</code></pre> <ul> <li>One of  the most promising developments in LLMs are LLM Agents i f you want multiple models to work together. </li> <li>The following links are worth going through</li> <li>A Survey on LLM-based Autonomous Agents \u2014 this is probably the most comprehensive overview of LLM based agents.</li> <li>autogen \u2014 is a framework that enables the development of LLM applications using multiple agents that can converse with each other to solve tasks.</li> <li>OpenAgents \u2014 an open platform for using and hosting language agents in the wild.</li> </ul>"},{"location":"#_24","title":"Home","text":"<pre><code>**Reinforcement Learning from Human Feedback (RLHF)**\n</code></pre> <ul> <li>As soon as you allow users access to your model, you start taking responsibility. </li> <li>What if it responds rudely? Or reveals bomb-making ingredients? To avoid this, check out these articles:</li> <li>Illustrating Reinforcement Learning from Human Feedback (RLHF) \u2014 an overview article that details the RLHF technology.</li> <li>RL4LMs \u2014 a modular RL library to fine-tune language models to human preferences.</li> <li>TRL \u2014 a set of tools to train transformer language models with Reinforcement Learning, from the Supervised Fine-tuning step (SFT), Reward Modeling step (RM) to the Proximal Policy Optimization (PPO) step.</li> </ul> <p>Summary:</p> <p></p> <ul> <li>The material covered today is broad but is the technology of the future. </li> <li>Junior Programmers, Artists, ML Engineers, Data Processing Analysts, Beginner Data Scientists, and practically every other digital job should be learning this technology. There is a lot of scope and opportunity. </li> <li>Generative AI is the future of the Digital Media World. Artists are feeling the impact today. A similar situation is looming for junior-level software engineers. </li> <li>But the solution is simple: Skill up! Help someone else Skill up! Regain Control!</li> </ul>"},{"location":"spanda-NaLLM-2/","title":"spanda NaLLM 2","text":"<p>&gt;&gt;&gt;&gt;&gt;  gd2md-html alert:  ERRORs: 0; WARNINGs: 1; ALERTS: 6.</p> <ul><li>See top comment block for details on ERRORs and WARNINGs. <li>In the converted Markdown or HTML, search for inline alerts that start with &gt;&gt;&gt;&gt;&gt;  gd2md-html alert:  for specific instances that need correction. <p>Links to alert messages:</p> <p>alert1 alert2 alert3 alert4 alert5 alert6</p> <p>&gt;&gt;&gt;&gt;&gt; PLEASE check and correct alert issues and delete this message and the inline alerts.</p> <p>This is the second blog post of Neo4j\u2019s NaLLM project. We started this project to explore, develop, and showcase practical uses of these LLMs in conjunction with Neo4j. As part of this project, we will construct and publicly display demonstrations in a GitHub repository, providing an open space for our community to observe, learn, and contribute. Additionally, we are writing about our findings in a blog post. You can find the first and third blog posts here:</p> <ul> <li>Harnessing LLMs With Neo4j</li> <li>Multi-Hop Question Answering</li> </ul>"},{"location":"spanda-NaLLM-2/#harnessing-large-language-models-with-neo4j","title":"Harnessing Large Language Models with Neo4j","text":""},{"location":"spanda-NaLLM-2/#episode-1-exploring-real-world-use-cases","title":"Episode 1 \u2014 Exploring Real-World Use Cases","text":"<p>medium.com</p> <p>The first wave of hype for Large Language Models (LLMs) came from ChatGPT and similar web-based chatbots, where the models are so good at understanding and generating text that it shocked people, myself included.</p> <p>Many of us logged in and tested its ability to write haikus, motivational letters, or email responses. What became quickly apparent is that LLMs are not only good at generating creative context but also at solving typical natural language processing and other tasks.</p> <p>Shortly after the LLM hype started, people started considering integrating it into their applications. Unfortunately, if you simply develop a wrapper around an LLM API, there is a high chance your application will not be successful as it doesn\u2019t provide additional value.</p> <p>One major problem of LLMs is the so-called knowledge cutoff. The knowledge cutoff term indicates that LLMs are unaware of any events that happened after their training. For example, if you ask ChatGPT about an event in 2023, you will get the following response.</p> <p>&gt;&gt;&gt;&gt;&gt;  gd2md-html alert: inline image link here (to images/image1.png). Store image on your image server and adjust path/filename/extension if necessary. (Back to top)(Next alert)&gt;&gt;&gt;&gt;&gt; </p> <p></p> <p>ChatGPT\u2019s knowledge cutoff date. Image by author.</p> <p>The same problem will occur if you ask an LLM about any event not present in its training dataset. While the knowledge cutoff date is relevant for any publicly available information, the LLM doesn\u2019t have any knowledge about private or confidential information that might be available even before the knowledge cutoff date.</p> <p>For example, most companies have some confidential information that they don\u2019t share publicly but might be interested in having a custom LLM that could answer those questions. On the other hand, a lot of the publicly available information that the LLM is aware of might be already outdated.</p> <p>Therefore, updating and expanding the knowledge of an LLM is highly relevant today.</p> <p>Another problem with LLMs is that they are trained to produce realistic-sounding text, which might not be accurate. Some invalid information is more challenging to spot than others. Especially for missing data, it is very probable that the LLM will make up an answer that sounds convincing but is nonetheless wrong instead of admitting that it lacks the base facts in its training.</p> <p>For example, research or court citations might be easier to verify. A week ago, a lawyer got in trouble for blindly believing the court citations ChatGPT produced.</p>"},{"location":"spanda-NaLLM-2/#lawyer-apologizes-for-fake-court-citations-from-chatgpt-cnn-business","title":"Lawyer apologizes for fake court citations from ChatGPT | CNN Business","text":""},{"location":"spanda-NaLLM-2/#the-meteoric-rise-of-chatgpt-is-shaking-up-multiple-industries-including-law-a-lawyer-for-a-man-suing-avianca","title":"The meteoric rise of ChatGPT is shaking up multiple industries - including law. A lawyer for a man suing Avianca\u2026","text":"<p>edition.cnn.com</p> <p>I have also noticed that LLMs will consistently produce assertive, yet false information about any sort of IDs like the WikiData or other identification numbers.</p> <p>&gt;&gt;&gt;&gt;&gt;  gd2md-html alert: inline image link here (to images/image2.jpg). Store image on your image server and adjust path/filename/extension if necessary. (Back to top)(Next alert)&gt;&gt;&gt;&gt;&gt; </p> <p></p> <p>ChatGPT\u2019s hallucinations. Image by author.</p> <p>Since the response by ChatGPT is assertive, you might expect it to be accurate. However, the given WikiData id points to a farm in England. Therefore, you have to be very careful not to blindly believe everything that LLMs produce. Verifying answers or producing more accurate results from LLMs is another big problem that needs to be solved.</p> <p>Of course, LLMs have other problems, like bias, prompt injection, and others. However, we will not talk about them here. Instead, in this blog post, we will present and focus on the concepts of fine-tuning and retrieval-augmented LLMs and evaluate their pros and cons.</p>"},{"location":"spanda-NaLLM-2/#supervised-fine-tuning-of-an-llm","title":"Supervised Fine-Tuning of an LLM","text":"<p>Explaining how LLMs are trained is beyond the scope of this blog post. Instead, you can watch this incredible video by Andrej Karpathy to catch up on LLMs and learn about the different phases of LLM training.</p> <p>By fine-tuning an LLM, we refer to the supervised training phase, during which you provide additional question-answer pairs to optimize the performance of the Large Language Model (LLM).</p> <p>Additionally, we have identified two different use cases for fine-tuning an LLM.</p> <p>One use case is fine-tuning a model to update and expand its internal knowledge. \\ In contrast, the other use case is focused on fine-tuning a model for a specific task like text summarization or translating natural language to database queries.</p> <p>First, we will talk about the first use case, where we use fine-tuning techniques to update and expand the internal knowledge of an LLM.</p> <p>&gt;&gt;&gt;&gt;&gt;  gd2md-html alert: inline image link here (to images/image3.png). Store image on your image server and adjust path/filename/extension if necessary. (Back to top)(Next alert)&gt;&gt;&gt;&gt;&gt; </p> <p></p> <p>Supervised fine-tuning flow. Image by author. Icons from Flaticons.</p> <p>Usually, you want to avoid pre-training an LLM as the cost can be upwards of hundreds of thousands and even millions of dollars. A base LLM is pre-trained using a gigantic corpus of text corpus, frequently in the billions or even trillions of tokens.</p> <p>While the number of parameters of an LLM is vital, it is not the only parameter you should consider when selecting a base LLM. Besides the license, you should also consider the bias and toxicity of the pre-training dataset and the base LLM.</p> <p>After you have selected the base LLM, you can start the next step of fine-tuning it. The fine-tuning step is relatively cheap regarding computation cost due to available techniques like the LoRa and QLoRA.</p>"},{"location":"spanda-NaLLM-2/#using-lora-for-efficient-stable-diffusion-fine-tuning","title":"Using LoRA for Efficient Stable Diffusion Fine-Tuning","text":""},{"location":"spanda-NaLLM-2/#lora-low-rank-adaptation-of-large-language-models-is-a-novel-technique-introduced-by-microsoft-researchers-to-deal","title":"LoRA: Low-Rank Adaptation of Large Language Models is a novel technique introduced by Microsoft researchers to deal\u2026","text":"<p>huggingface.co</p> <p>However, constructing a training dataset is more complex and can get expensive. If you can not afford a dedicated team of annotators, it seems that the trend is to use an LLM to construct a training dataset to fine-tune your desired LLM (this is really meta).</p> <p>For example, Stanford\u2019s Alpaca training dataset was created using OpenAI\u2019s LLMs. The cost to produce 52 thousand training instructions was about 500 dollars, which is relatively cheap.</p>"},{"location":"spanda-NaLLM-2/#stanford-crfm","title":"Stanford CRFM","text":""},{"location":"spanda-NaLLM-2/#we-introduce-alpaca-7b-a-model-fine-tuned-from-the-llama-7b-model-on-52k-instruction-following-demonstrations-on-our","title":"We introduce Alpaca 7B, a model fine-tuned from the LLaMA 7B model on 52K instruction-following demonstrations. On our\u2026","text":"<p>crfm.stanford.edu</p> <p>On the other hand, the Vicuna model was fine-tuned by using the ChatGPT conversations users posted on ShareGPT.com.</p>"},{"location":"spanda-NaLLM-2/#vicuna-an-open-source-chatbot-impressing-gpt-4-with-90-chatgpt-quality-lmsys-org","title":"Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90%* ChatGPT Quality | LMSYS Org","text":""},{"location":"spanda-NaLLM-2/#by-the-vicuna-team-mar-30-2023-we-introduce-vicuna-13b-an-open-source-chatbot-trained-by-fine-tuning-llama-on","title":"by: The Vicuna Team, Mar 30, 2023 We introduce Vicuna-13B, an open-source chatbot trained by fine-tuning LLaMA on\u2026","text":"<p>lmsys.org</p> <p>There is also a relatively fresh project by H2O called WizardLM, which is designed to turn documents into question-answer pairs that can be used to fine-tune an LLM.</p> <p>We haven\u2019t found any recent articles describing how to use a knowledge graph to prepare good question-answer pairs that can be used to fine-tune an LLM.</p> <p>This is an area that we plan to explore during the NaLLM project. We have some ideas for utilizing LLMs to construct question-answer pairs from a knowledge graph context.</p> <p>However, there are a lot of unknowns at the moment. \\ For example, can you provide two different answers to the same question, and the LLM then somehow combines them in its internal knowledge store?</p> <p>Another consideration is that some information in a knowledge graph is not relevant without considering its relationships. Therefore, do we have to pre-define relevant queries, or is there a more generic way to go about it? Or can we use the node-relationship-node patterns representing subject-predicate-object expressions to generate relevant pairs?</p> <p>These are some of the questions we aim to answer in upcoming blog posts.</p> <p>Imagine that you somehow managed to produce a training dataset containing question-answer pairs based on the information stored in your knowledge graph. As a result, the LLM now includes updated knowledge.</p> <p>However, fine-tuning the model didn\u2019t solve the knowledge cutoffs problem since it only pushed the knowledge cutoff to a later date.</p> <p>Therefore, we recommend updating the internal knowledge of an LLM through fine-tuning techniques _only for slowly changing _or updating data. For example, you could use a fine-tuned model to provide tourist information.</p> <p>However, you would run into troubles the second you would want to include special time-dependent (real-time) or personalized promotions in the responses. Similarly, fine-tuned models are not ideal for analytical workflows where you would ask how many new customers the company gained over the last week.</p> <p>At the moment, fine-tuning approaches can help mitigate hallucinations but cannot completely eliminate them. One problem is that the LLMs do not cite their sources when providing answers. Therefore, you have no idea if the answer came from pre-training data, fine-tuning dataset, or was made up by the LLM. Additionally, there might be another possible falsehood source if you use an LLM to create the fine-tuning dataset.</p> <p>Lastly, a fine-tuned model cannot automatically provide different responses depending on the user making the questions. Likewise, there is no concept of access restrictions, meaning that anybody interacting with the LLM has access to all of its information.</p>"},{"location":"spanda-NaLLM-2/#retrieval-augmented-generation","title":"Retrieval-Augmented Generation","text":"<p>Large language models perform remarkably well in natural language applications like</p> <ul> <li>Text summarization,</li> <li>Extracting relevant information,</li> <li>Disambiguation of entities</li> <li>Translating from one language to another, or even</li> <li>Converting natural language into database queries or scripting code.</li> </ul> <p>Moreover, previously NLP models were most often domain and task-specific, meaning that you would most likely need to train a custom natural language model depending on your use case and domain. However, thanks to the generalization capabilities of LLMs, a single model can be applied to solve various collections of tasks.</p> <p>We have observed quite a strong trend in using retrieval-augmented LLMs, where instead of using LLMs to access its internal knowledge, you use the LLM as a natural language interface to your company\u2019s or private information.</p> <p>&gt;&gt;&gt;&gt;&gt;  gd2md-html alert: inline image link here (to images/image4.png). Store image on your image server and adjust path/filename/extension if necessary. (Back to top)(Next alert)&gt;&gt;&gt;&gt;&gt; </p> <p></p> <p>Retrieval-augmented generation. Image by author. Icons from Flaticons.</p> <p>The retrieval augmented approach uses the LLM to generate an answer based on the additionally provided relevant documents from your data source.</p> <p>Therefore, you don\u2019t rely on internal knowledge of the LLM to produce answers. Instead, the LLM is used only for extracting relevant information from documents you passed in and summarizing it.</p>"},{"location":"spanda-NaLLM-2/#chatgpt-plugins","title":"ChatGPT plugins","text":"<p>openai.com</p> <p>For example, the ChatGPT plugins can be thought of as a retrieval-augmented approach to LLM applications. The ChatGPT interface with a browsing plugin enabled allows the LLM to search the internet to access up-to-date information and use it to construct the final answer.</p> <p>&gt;&gt;&gt;&gt;&gt;  gd2md-html alert: inline image link here (to images/image5.png). Store image on your image server and adjust path/filename/extension if necessary. (Back to top)(Next alert)&gt;&gt;&gt;&gt;&gt; </p> <p></p> <p>ChatGPT with browsing plugin. Image by author.</p> <p>In this example, ChatGPT was able to answer who won the Oscar for various categories in 2023. But, remember, the cutoff knowledge date for ChatGPT is 2021, so it couldn\u2019t know who won the 2023 Oscars from its internal knowledge. Therefore, it accessed external information through the browsing plugin, which allowed it to answer the question with up-to-date information. Those plugins present an integrated augmentation mechanism inside the OpenAI platform.</p> <p>If you have been watching the LLM space, you might have heard of the LangChain library.</p>"},{"location":"spanda-NaLLM-2/#getting-started-with-langchain-a-beginners-guide-to-building-llm-powered-applications","title":"Getting Started with LangChain: A Beginner\u2019s Guide to Building LLM-Powered Applications","text":""},{"location":"spanda-NaLLM-2/#a-langchain-tutorial-to-build-anything-with-large-language-models-in-python","title":"A LangChain tutorial to build anything with large language models in Python","text":"<p>towardsdatascience.com</p> <p>The LangChain library can be used to allow LLMs to access real-time information from various sources like Google Search, vector databases, or knowledge graphs. For example, LangChain has added a Cypher Search chain, which converts natural language questions into a Cypher statement, uses it to retrieve information from the Neo4j database, and constructs a final answer based on the provided information.</p>"},{"location":"spanda-NaLLM-2/#langchain-has-added-cypher-search","title":"LangChain has added Cypher Search","text":""},{"location":"spanda-NaLLM-2/#with-the-langchain-library-you-can-conveniently-generate-cypher-queries-enabling-an-efficient-retrieval-of","title":"With the LangChain library, you can conveniently generate Cypher queries, enabling an efficient retrieval of\u2026","text":"<p>towardsdatascience.com</p> <p>With the Cypher Search chain, an LLM is not only used to construct a final answer but also to translate a natural language question into a Cypher query.</p> <p>&gt;&gt;&gt;&gt;&gt;  gd2md-html alert: inline image link here (to images/image6.png). Store image on your image server and adjust path/filename/extension if necessary. (Back to top)(Next alert)&gt;&gt;&gt;&gt;&gt; </p> <p></p> <p>Cypher search in LangChain. Image by author.</p> <p>Another popular library for retrieval-augmented LLM workflows is LlamaIndex (GPT Index). LlamaIndex is a comprehensive data framework aimed at enhancing the performance of Large Language Models (LLMs) by enabling them to leverage private or custom data.</p>"},{"location":"spanda-NaLLM-2/#llamaindex-on-twiml-ai-a-distilled-summary-using-llamaindex","title":"LlamaIndex on TWIML AI: A Distilled Summary (using LlamaIndex)","text":""},{"location":"spanda-NaLLM-2/#overview","title":"Overview","text":"<p>medium.com</p> <p>Firstly, LlamaIndex offers data connectors that facilitate the ingestion of a variety of data sources and formats, encompassing everything from APIs, PDFs, and documents to SQL or graph data.</p> <p>This feature allows for an effortless integration of existing data into the LLM. Secondly, it provides efficient mechanisms to structure the ingested data using indices and graphs, ensuring the data is suitably arranged for use with LLMs. In addition, it includes an advanced retrieval and query interface, which enables users to input an LLM prompt and receive back a context-retrieved, knowledge-augmented output.</p> <p>The idea behind retrieval-augmented LLM applications like ChatGPT Plugins and LangChain is to avoid relying on internal LLM knowledge only to generate answers. Instead, LLMs are used to solve tasks like constructing database queries from natural language and constructing answers based on externally provided information or by utilizing plugins/agents for retrieval.</p> <p>The retrieval-augmented approach has some clear advantages over the fine-tuning approach:</p> <ul> <li>The answer can cite its sources of information, which allows you to validate the information and potentially change or update the underlying information based on requirements</li> <li>Hallucinations are more unlikely to occur as you don\u2019t rely on the internal knowledge of an LLM to answer the question and only use information that is provided in the relevant documents</li> <li>Changing, updating, and maintaining the underlying information the LLM uses is easier as you transform the problem from LLM maintenance to a database maintenance, querying and context construction problem</li> <li>Answers can be personalized based on the user context, or their access permission</li> </ul> <p>On the other hand, you should consider the following limitations when using the retrieval-augmented approach:</p> <ul> <li>The answers are only as good as the smart search tool</li> <li>The application needs access to your specific knowledge base, either that be a database or other data stores</li> <li>Completely disregarding the internal knowledge of the language model limits the number of questions that can be answered</li> <li>Sometimes LLMs fail to follow instructions, so there is a risk that the context might be ignored or hallucinations occur if no relevant answer data is found in the context.</li> </ul>"},{"location":"spanda-NaLLM-2/#summary","title":"Summary","text":"<p>This blog post delves into the limitations of Large Language Models (LLMs), such as</p> <ul> <li>Knowledge cutoff,</li> <li>Hallucinations, and</li> <li>The lack of user customization.</li> </ul> <p>To overcome these, we explored two concepts, namely, fine-tuning and retrieval-augmented use of LLMs.</p> <p>Fine-tuning an LLM involves the supervised training phase, where question-answer pairs are provided to optimize the performance of the LLM. This can be used to update and expand the LLM\u2019s internal knowledge or fine-tune it for a specific task. However, fine-tuning fails to solve the knowledge cutoff issue as it simply pushes the cutoff to a later date. It also cannot fully eliminate hallucinations. Therefore, we recommend using the fine-tuning approach for slowly changing datasets where some hallucinations are allowed. Since fine-tuning LLMs is relatively new, we are eager to learn more about fine-tuning approaches and best practices.</p> <p>The second approach to overcome the limitations of LLMs is the so-called retrieval-augmented generation, where the LLM serves as a natural language interface to access external information, thereby not relying only on its internal knowledge to produce answers. Advantages of the retrieval-augmented approach include source-citing, negligible hallucinations, ease of changing and updating information, and personalization. \\ However, it relies heavily on the intelligent search tool to retrieve relevant information and requires access to the user\u2019s knowledge base. Furthermore, it can only answer queries provided it has the information required to address the question.</p> <p>Keep an eye out for updates from our team as we progress the development of this project, all of which will be openly documented on our GitHub repository.</p>"},{"location":"spanda-NaLLM-3/","title":"spanda NaLLM 3","text":"<p>&gt;&gt;&gt;&gt;&gt;  gd2md-html alert:  ERRORs: 0; WARNINGs: 1; ALERTS: 10.</p> <ul><li>See top comment block for details on ERRORs and WARNINGs. <li>In the converted Markdown or HTML, search for inline alerts that start with &gt;&gt;&gt;&gt;&gt;  gd2md-html alert:  for specific instances that need correction. <p>Links to alert messages:</p> <p>alert1 alert2 alert3 alert4 alert5 alert6 alert7 alert8 alert9 alert10</p> <p>&gt;&gt;&gt;&gt;&gt; PLEASE check and correct alert issues and delete this message and the inline alerts.</p> <p>This is the third blog post of Neo4j\u2019s NaLLM project. We started this project to explore, develop, and showcase practical uses of these LLMs in conjunction with Neo4j. As part of this project, we will construct and publicly display demonstrations in a GitHub repository, providing an open space for our community to observe, learn, and contribute. Additionally, we have been writing about our findings in blog posts. You can see the previous two blog posts here:</p> <ul> <li>Harnessing LLMs With Neo4j</li> <li>Fine-Tuning vs Retrieval-Augmented Generation</li> </ul> <p>&gt;&gt;&gt;&gt;&gt;  gd2md-html alert: inline image link here (to images/image1.png). Store image on your image server and adjust path/filename/extension if necessary. (Back to top)(Next alert)&gt;&gt;&gt;&gt;&gt; </p> <p></p> <p>Midjourney\u2019s idea of an investigative board.</p> <p>In the previous blog post, we learned about the retrieval-augmented approach to overcome the limitations of Large Language Models (LLMs), such as hallucinations and limited knowledge. The idea behind the retrieval-augmented approach is to reference external data at question time and feed it to an LLM to enhance its ability to generate accurate and relevant answers.</p> <p>&gt;&gt;&gt;&gt;&gt;  gd2md-html alert: inline image link here (to images/image2.png). Store image on your image server and adjust path/filename/extension if necessary. (Back to top)(Next alert)&gt;&gt;&gt;&gt;&gt; </p> <p></p> <p>Retrieval-augmented approach to LLM applications. Image by author.</p> <p>When a user asks a question, an intelligent search tool looks for relevant information in the provided Knowledge bases. For example, you might have encountered instances of searching for relevant information within PDFs or a company\u2019s documentation. Most of those examples use vector similarity search to identify which chunks of text might contain relevant data to answer the user\u2019s question accurately. The implementation is relatively straightforward.</p> <p>&gt;&gt;&gt;&gt;&gt;  gd2md-html alert: inline image link here (to images/image3.png). Store image on your image server and adjust path/filename/extension if necessary. (Back to top)(Next alert)&gt;&gt;&gt;&gt;&gt; </p> <p></p> <p>RAG applications using vector similarity search. Image by author.</p> <p>The PDFs or the documentation are first split into multiple chunks of text. Some different strategies include how large the text chunks should be and if there should be any overlap between them. In the next step, vector representations of text chunks are generated by using any of the available text embedding models. That is all the preprocessing needed to perform a vector similarity search at query time. The only step left is to encode the user input as a vector at query time and use cosine or any other similarity to compare the distance between the user input and the embedded text chunks. Most frequently, you will see that the top three most similar documents are returned to provide the context to the LLM to enhance its capability to generate accurate answers. This approach works fairly well when the vector search can produce relevant chunks of text.</p> <p>However, simple vector similarity search might not be sufficient when the LLM needs information from multiple documents or even just multiple chunks to generate an answer.</p> <p>For example, consider the following question:</p> <p>Did any of the former OpenAI employees start their own company?</p> <p>If you think about it, this question can be broken down into two questions.</p> <ul> <li>Who are the former employees of OpenAI?</li> <li>Did any of them start their own company?</li> </ul> <p>&gt;&gt;&gt;&gt;&gt;  gd2md-html alert: inline image link here (to images/image4.png). Store image on your image server and adjust path/filename/extension if necessary. (Back to top)(Next alert)&gt;&gt;&gt;&gt;&gt; </p> <p></p> <p>Information spanning across multiple documents. Image by author.</p> <p>Answering these types of questions is a multi-hop question-answering task, where a single question can be broken down into multiple sub-questions and can require numerous documents to be provided to the LLM to generate an accurate answer.</p> <p>The above-mentioned workflow of simply chunking and embeddings documents in a database and then using plain vector similarity search might struggle with multi-hop questions due to:</p> <ul> <li>Repeated information in top N documents: The provided documents are not guaranteed to contain complementary and complete information needed to answer a question. For example, the top three similar documents might all mention that Shariq worked at OpenAI and possibly founded a company while completely ignoring all the other former employees that became founders</li> <li>Missing reference information: Depending on the chunk sizes, you might lose the reference to the entities in the documents. This can be partially solved by chunk overlaps. However, there are also examples where the references point to another document, so some sort of co-reference resolution or other preprocessing would be needed.</li> <li>Hard to define ideal N number of retrieved documents: Some questions require more documents to be provided to an LLM to accurately answer the question, while in other situations, a large number of provided documents would only increase the noise (and cost).</li> </ul> <p>&gt;&gt;&gt;&gt;&gt;  gd2md-html alert: inline image link here (to images/image5.png). Store image on your image server and adjust path/filename/extension if necessary. (Back to top)(Next alert)&gt;&gt;&gt;&gt;&gt; </p> <p></p> <p>An example where the similarity search might return some duplicated information, while other relevant information could be ignored due to a low K number of retrieved information or embedding distance. Image by the author.</p> <p>Therefore, a plain vector similarity search might struggle with multi-hop questions. However, we can employ multiple strategies to attempt to answer multi-hop questions requiring information from various documents.</p>"},{"location":"spanda-NaLLM-3/#knowledge-graph-as-condensed-information-storage","title":"Knowledge Graph as Condensed Information Storage","text":"<p>If you are paying close attention to the LLM space, you might have come across the idea of using various techniques to condense information for it to be more easily accessible during query time. For example, you could use an LLM to provide a summary of documents and then embed and store the summaries instead of the actual documents. Using this approach, you could remove a lot of noise, get better results, and worry less about prompt token space.</p> <p>Interestingly, you could conduct the contextual summarization at ingestion or perform it during the query time. Contextual compression during query time is interesting as the context is picked that is relevant to the provided question, so it is a bit more guided. However, the heavier the workload during the query time, the worse the expected user latency will be. Therefore, it is recommended to move as much of the workload to ingestion time as possible to improve latency and avoid other runtime issues.</p> <p>The same approach can be applied to summarize conversation history to avoid running into token limit problems.</p> <p>I haven\u2019t seen any articles about combining and summarizing multiple documents as a single record. The problem is probably that there are too many combinations of documents that we could merge and summarize. Therefore, it is perhaps too costly to process all the combinations of documents at ingestion time. \\ However, a knowledge graph can help here too.</p> <p>The process of extracting structured information in the form of entities and relationships from unstructured text has been around for some time and is better known as the information extraction pipeline. The beauty of combining an information extraction pipeline with knowledge graphs is that you can process each document individually, and the information from different records gets connected when the knowledge graph is constructed or enriched.</p> <p>&gt;&gt;&gt;&gt;&gt;  gd2md-html alert: inline image link here (to images/image6.png). Store image on your image server and adjust path/filename/extension if necessary. (Back to top)(Next alert)&gt;&gt;&gt;&gt;&gt; </p> <p></p> <p>Extracting entities and relationships from text to construct a knowledge graph. Image by author.</p> <p>The knowledge graph used nodes and relationships to represent data. In this example, the first document provided the information that Dario and Daniela used to work at OpenAI, while the second document offered information about their Anthropic startup. Each record was processed individually, yet the knowledge graph representation connects the data and makes it easy to answer questions spanning across multiple documents.</p> <p>Most of the newer approaches using LLMs to answer multi-hop questions we encountered focus on solving the task at query time. However, we believe that many multi-hop question-answering issues can be solved by preprocessing data before ingestion and connecting it in a knowledge graph. The information extraction pipeline can be performed using LLMs or custom text domain models.</p> <p>In order to retrieve information from the knowledge graph at query time, we have to construct an appropriate Cypher statement. Luckily, LLMs are pretty good at translating natural language to Cypher graph-query language.</p> <p>&gt;&gt;&gt;&gt;&gt;  gd2md-html alert: inline image link here (to images/image7.png). Store image on your image server and adjust path/filename/extension if necessary. (Back to top)(Next alert)&gt;&gt;&gt;&gt;&gt; </p> <p></p> <p>Using knowledge graphs as part of retrieval-augmented LLM applications. Image by author.</p> <p>In this example, the smart search uses an LLM to generate an appropriate Cypher statement to retrieve relevant information from a knowledge graph. The relevant information is then passed to another LLM call, which uses the original question and the provided information to generate an answer. In practice, you could use different LLMs for generating Cypher statements and answers or use various prompts on a single LLM.</p>"},{"location":"spanda-NaLLM-3/#combining-graph-and-textual-data","title":"Combining Graph and Textual Data","text":"<p>Sometimes, you might want to combine textual and graph data to find relevant information. For example, consider the following question:</p> <p>What is the latest news about Prosper Robotics founders?</p> <p>In this example, you might want to identify the Prosper Robotics founders using the knowledge graph structure and retrieve the latest articles mentioning them.</p> <p>&gt;&gt;&gt;&gt;&gt;  gd2md-html alert: inline image link here (to images/image8.png). Store image on your image server and adjust path/filename/extension if necessary. (Back to top)(Next alert)&gt;&gt;&gt;&gt;&gt; </p> <p></p> <p>Knowledge graph with explicit links between structured information and unstructured text. Image by author.</p> <p>To answer the question about the latest news about Prosper Robotics founders, you would start from the Prosper Robotics node, traverse to its founders, and then retrieve the latest articles mentioning them.</p> <p>A knowledge graph can be used to represent structured information about entities and their relationships, as well as unstructured text as node properties. Additionally, you could employ natural language techniques like named entity recognition to connect unstructured information to relevant entities in the knowledge graph, as shown with the MENTIONS relationship.</p> <p>We believe that the future of retrieval-augmented generation applications is utilizing both structured and unstructured information to generate accurate answers. Therefore, a knowledge graph is a perfect solution because you can store both structured and unstructured data and connect them with explicit relationships, making information more accessible and easier to find.</p> <p>&gt;&gt;&gt;&gt;&gt;  gd2md-html alert: inline image link here (to images/image9.png). Store image on your image server and adjust path/filename/extension if necessary. (Back to top)(Next alert)&gt;&gt;&gt;&gt;&gt; </p> <p></p> <p>Using Cypher and vector similarity search to retrieve relevant information from a knowledge graph. Image by author.</p> <p>When the knowledge graph contains structured and unstructured data, the smart search tool could utilize Cypher queries or vector similarity search to retrieve relevant information. In some cases, you could also use a combination of the two. For example, you could start with a Cypher query to identify relevant documents and then use vector similarity search to find specific information within those documents.</p>"},{"location":"spanda-NaLLM-3/#using-knowledge-graphs-in-chain-of-thought-flow","title":"Using Knowledge Graphs in Chain-of-Thought Flow","text":"<p>Another very exciting development around LLMs is the so-called chain-of-thought question answering, especially with LLM agents. The idea behind LLM agents is that they can decompose questions into multiple steps, define a plan, and use any of the provided tools. In most cases, the agent tools are APIs or knowledge bases that the agent can access to retrieve additional information. Let\u2019s again consider the following question:</p> <p>What is the latest news about Prosper Robotics founders?</p> <p>&gt;&gt;&gt;&gt;&gt;  gd2md-html alert: inline image link here (to images/image10.png). Store image on your image server and adjust path/filename/extension if necessary. (Back to top)(Next alert)&gt;&gt;&gt;&gt;&gt; </p> <p></p> <p>No explicit links between knowledge graph entities and unstructured text. Image by author.</p> <p>Suppose you don\u2019t have explicit connections between articles and entities they mention. The articles and entities could even be in separate databases. In this case, an LLM agent using chain-of-thought flow would be very helpful. First, the agent would decompose the question into sub-questions.</p> <ul> <li>Who are the founders of Prosper Robotics?</li> <li>What is the latest news about them?</li> </ul> <p>Now, an agent could decide which tool to use. Suppose we provide it with a knowledge graph access that it can use to retrieve structured information. Therefore, an agent could choose to retrieve the information about the founders of Prosper Robotics from a knowledge graph. As we already know, the founder of Prosper Robotics is Shariq Hashme. Now that the first question was answered, the agent could rewrite the second subquestion as:</p> <ul> <li>What is the latest news about Shariq Hashme?</li> </ul> <p>The agent could use any of the available tools to answer the subsequent question. The tools can range from knowledge graphs, document or vector databases, various APIs, and more. Having access to structured information allows LLM applications to perform various analytics workflows where aggregation, filtering, or sorting is required. Consider the following questions:</p> <ul> <li>Which company with a solo founder has the highest valuation?</li> <li>Who founded the most companies?</li> </ul> <p>Plain vector similarity search can struggle with these types of analytical questions since it searches through unstructured text data, making it hard to sort or aggregate data. Therefore, a combination of structured and unstructured data is probably the future of retrieval-augmented LLM applications. Additionally, as we have seen, knowledge graphs are also ideal for representing connected information and, consequently, multi-hop queries.</p> <p>While the chain-of-thought is a fascinating development around LLMs as it shows how an LLM can reason, it is not the most user-friendly as the response latency can be high due to multiple LLM calls. However, we are still very excited to understand more about incorporating knowledge graphs into chain-of-thought flows for various use cases.</p>"},{"location":"spanda-NaLLM-3/#summary","title":"Summary","text":"<p>Retrieval-augmented generation applications often require retrieving information from multiple sources to generate accurate answers. While textual summarization can be challenging, representing information in a graph format can offer several advantages.</p> <p>By processing each document separately and connecting them in a knowledge graph, we can construct a structured representation of the information. This approach allows for easier traversal and navigation through interconnected documents, enabling multi-hop reasoning to answer complex queries. Furthermore, constructing the knowledge graph during the ingestion phase reduces the workload during query time, resulting in improved latency.</p> <p>Another advantage of using a knowledge graph is its ability to store both structured and unstructured information. This flexibility makes a knowledge graphs suitable for a wide range of language model (LLM) applications, as it can handle various data types and relationships between entities. The graph structure provides a visual representation of the knowledge, facilitating transparency and interpretability for both developers and users.</p> <p>Overall, leveraging knowledge graphs in retrieval-augmented generation applications offers benefits such as improved query efficiency, multi-hop reasoning capabilities, and support for structured and unstructured information.</p> <p>Keep an eye out for updates from our team as we progress the development of this project, all of which will be openly documented on our GitHub repository.</p>"},{"location":"spanda-NaLLM-4/","title":"spanda NaLLM 4","text":"<p>&gt;&gt;&gt;&gt;&gt;  gd2md-html alert:  ERRORs: 0; WARNINGs: 1; ALERTS: 8.</p> <ul><li>See top comment block for details on ERRORs and WARNINGs. <li>In the converted Markdown or HTML, search for inline alerts that start with &gt;&gt;&gt;&gt;&gt;  gd2md-html alert:  for specific instances that need correction. <p>Links to alert messages:</p> <p>alert1 alert2 alert3 alert4 alert5 alert6 alert7 alert8</p> <p>&gt;&gt;&gt;&gt;&gt; PLEASE check and correct alert issues and delete this message and the inline alerts.</p> <p>This is the fourth blog post of Neo4j\u2019s NaLLM project. We started this project to explore, develop, and showcase practical uses of these LLMs in conjunction with Neo4j. As part of this project, we will construct and publicly display demonstrations in a GitHub repository, providing an open space for our community to observe, learn, and contribute. Additionally, we are writing about our findings in a blog post. You can read the previous three blog posts here:</p> <ul> <li>Harnessing LLMs with Neo4j</li> <li>Fine-tuning vs. Retrieval-augmented generation</li> <li>Multi-hop Question Answering</li> </ul> <p>&gt;&gt;&gt;&gt;&gt;  gd2md-html alert: inline image link here (to images/image1.png). Store image on your image server and adjust path/filename/extension if necessary. (Back to top)(Next alert)&gt;&gt;&gt;&gt;&gt; </p> <p></p> <p>Graph data analyst as imagined by Midjourney.</p> <p>Large Language Models (LLMs) have significantly changed data accessibility to the average person. Less than a year ago, accessing the company\u2019s data required technical skills involving proficiency in numerous dashboarding tools or even diving into the intricacies of a database query language. Yet, with the rise of LLMs like ChatGPT, the wealth of knowledge hidden within private databases or accessible via various APIs is now more readily available than ever with the rise of so-called retrieval-augmented LLM applications.</p> <p>&gt;&gt;&gt;&gt;&gt;  gd2md-html alert: inline image link here (to images/image2.png). Store image on your image server and adjust path/filename/extension if necessary. (Back to top)(Next alert)&gt;&gt;&gt;&gt;&gt; </p> <p></p> <p>Retrieval-augmented generation application. Image by author.</p> <p>The idea behind retrieval-augmented applications is to retrieve additional information from various sources to allow the LLM to generate better and more accurate results. It seems OpenAI has also picked up on this trend as they introduced OpenAI functions recently. The new OpenAI models are trained to use provide parameters to functions (or what other libraries call tools), whose signatures and descriptions are passed in the context, to retrieve additional information at query time if needed.</p> <p>We have observed a strong bias for vector similarity search in retrieval-augmented applications. If you opened Twitter or LinkedIn in the past three months, you might have seen the various \u201cChat with your PDFs\u201d applications. In those examples, the implementation is relatively straightforward. The text is extracted from PDFs, split into chunks if needed, and finally stored in a vector database along with its text embedding representations.</p> <p>The barrier to entry with these types of applications is low, especially if you are dealing with small amounts of data. It is fascinating that so many articles giving the impression that only vector databases are relevant for retrieval-augmented applications are published nowadays.</p> <p>While there is immense power in vector similarity-based information retrieval from unstructured text, we believe that structured information has an important role to play in LLM applications.</p> <p>Last time we wrote about multi-hop question answering and how knowledge graphs can help solve problems of retrieving information from multiple documents to generate an accurate answer. Additionally, we hinted that vector similarity search is not designed for analytics workflows, where we rely on structured information.</p>"},{"location":"spanda-NaLLM-4/#knowledge-graphs-llms-multi-hop-question-answering","title":"Knowledge Graphs &amp; LLMs: Multi-Hop Question Answering","text":""},{"location":"spanda-NaLLM-4/#retrieve-information-that-spans-across-multiple-documents","title":"Retrieve information that spans across multiple documents","text":"<p>medium.com</p> <p>For example, questions like:</p> <ul> <li>Who could introduce me to Emil Eifr\u00e9m (CEO of Neo4j)?</li> <li>How is ALOX5 gene related to Crohn\u2019s disease?</li> <li>When we have a particular microservice outage, how does it affect our products?</li> <li>How does a flight delay propagate through the network?</li> <li>Which users can be credited for a social media post virality?</li> </ul> <p>All these questions require highly-connected information to be able to answer the question accurately. For example, to learn who can introduce you to Emil, you need information about relationships between people.</p> <p>On the other hand, you need to map dependencies between your microservices and products in order to evaluate the scale and severity of a particular microservice failure.</p> <p>In this blog post, we will introduce some of the frequent use cases of real-time graph analytics that you might want to implement into your LLM applications.</p>"},{"location":"spanda-NaLLM-4/#finding-shortest-paths","title":"Finding (Shortest) Paths","text":"<p>Relationships are first-class citizens in native graph databases. Although knowledge graphs allow you to perform typical aggregations and filtering to answer questions like \u201cHow many customers did we get this week?\u201d, we will focus more on analytical use cases where traversing the relationships is the main component. One such example is finding the shortest or all possible paths between data points. For example, to answer the question:</p> <p>Who could introduce me to Emil Eifrem?</p> <p>We would have to find the shortest path between myself and Emil Eifrem in the graph.</p> <p>&gt;&gt;&gt;&gt;&gt;  gd2md-html alert: inline image link here (to images/image3.png). Store image on your image server and adjust path/filename/extension if necessary. (Back to top)(Next alert)&gt;&gt;&gt;&gt;&gt; </p> <p></p> <p>Single shortest path. Image by author.</p> <p>Another use case where finding the shortest paths in real-time would come in handy is in any sort of transportation, logistics, or routing application. In these applications, you might want to evaluate the top N shortest paths to ensure some fallback plan if something unexpected happens.</p> <p>&gt;&gt;&gt;&gt;&gt;  gd2md-html alert: inline image link here (to images/image4.png). Store image on your image server and adjust path/filename/extension if necessary. (Back to top)(Next alert)&gt;&gt;&gt;&gt;&gt; </p> <p></p> <p>Top two shortest paths between stops in Rome. Image by author.</p> <p>This image visualizes the top 2 shortest paths between two stops in Rome. Such shortest paths could be optimized for distance, time, cost, or a combination.</p> <p>Another domain where finding paths between data points in your LLM applications is the biomedical domain. In the biomedical domain, you are dealing with genes, proteins, diseases, drugs, and more. What\u2019s perhaps more important is that these entities do not exist in isolation but have complex, often multilayered relationships with each other.</p> <p>For instance, a gene may be associated with multiple diseases, a protein may interact with numerous other proteins, a disease might be treatable by a variety of drugs, and a drug could have multiple effects on different genes and proteins.</p> <p>Given the staggering amount of biomedical data available, the number of potential relationships between these data points is enormous and is a great candidate to be represented as a knowledge graph.</p> <p>&gt;&gt;&gt;&gt;&gt;  gd2md-html alert: inline image link here (to images/image5.png). Store image on your image server and adjust path/filename/extension if necessary. (Back to top)(Next alert)&gt;&gt;&gt;&gt;&gt; </p> <p></p> <p>All shortest paths between Crohn\u2019s disease and ALOX5 gene using Hetionet dataset. Image by author.</p> <p>Biomedical knowledge graph can support LLM applications where users are be interested in answering questions like</p> <p>How is ALOX5 gene related to Crohn\u2019s disease?</p> <p>While most LLM applications we see today generate answers as a natural language, there is also an excellent opportunity for returning responses in the form of line, bar, or even network visualizations. Often the LLM can even return the configuration structure needed for the charting libraries.</p>"},{"location":"spanda-NaLLM-4/#information-propagating-through-network","title":"Information Propagating Through Network","text":"<p>Another strong knowledge graph fit is domains with networks of dependencies. For example, you could have a knowledge graph containing the complete microservice architecture of your system. Such a knowledge graph would allow you to power a DevOps chatbot that would enable you to evaluate the architecture in real-time and perform what-if analysis.</p> <p>&gt;&gt;&gt;&gt;&gt;  gd2md-html alert: inline image link here (to images/image6.png). Store image on your image server and adjust path/filename/extension if necessary. (Back to top)(Next alert)&gt;&gt;&gt;&gt;&gt; </p> <p></p> <p>Microservices &amp; People graph. Screenshot from https://www.youtube.com/watch?v=_qakAUjXiek&amp;t=2517s</p> <p>Also Rhys Evans presented how the Financial Times manages their infrastructure as a graph.</p> <p>Another domain that comes to mind is the supply chain.</p> <p>&gt;&gt;&gt;&gt;&gt;  gd2md-html alert: inline image link here (to images/image7.png). Store image on your image server and adjust path/filename/extension if necessary. (Back to top)(Next alert)&gt;&gt;&gt;&gt;&gt; </p> <p></p> <p>Image from https://neo4j.com/blog/graphs-in-automotive-and-manufacturing/.</p> <p>Incorporating supply chain data into knowledge graphs can significantly enhance the capabilities of large language applications. This approach allows us to structure complex supply chain information into nodes and relationships, thereby generating a holistic picture of how materials, components, and products flow from suppliers to customers. The inherent interconnections and dependencies become evident and analyzable.</p> <p>For language applications, this enables deeper context understanding and knowledge generation. For instance, an AI model like ChatGPT can leverage this data structure to produce more accurate and insightful responses about supply chain scenarios, disruptions, or management strategies. It could comprehend and explain the ripple effects of a shortage of a certain component, predict potential bottlenecks, or suggest optimization strategies.</p> <p>By aligning the intricacies of supply chain dynamics with the cognitive abilities of AI, we can bolster the functionality and value of large language applications in a multitude of industrial and commercial contexts.</p>"},{"location":"spanda-NaLLM-4/#social-network-analysis-and-data-science","title":"Social Network Analysis and Data Science","text":"<p>What if your company chatbot went beyond documentation and helped deliver insights and recommendations as part of the people analytics?</p> <p>&gt;&gt;&gt;&gt;&gt;  gd2md-html alert: inline image link here (to images/image8.png). Store image on your image server and adjust path/filename/extension if necessary. (Back to top)(Next alert)&gt;&gt;&gt;&gt;&gt; </p> <p></p> <p>Image from https://neo4j.com/blog/neo4j-critical-aspect-human-capital-management-hcm/.</p> <p>Knowledge graphs in HCM can serve as an invaluable tool in driving people analytics within a company, primarily by creating a robust, interconnected system of information that allows for a deep, holistic understanding of employee behavior, skills, competencies, interactions, and performance. Essentially, a knowledge graph captures and links complex employee data \u2014 including demographic information, role history, project involvement, performance indicators, and skillsets \u2014 allowing for multi-faceted analysis.</p> <p>This combination of connected data and ML powered tools enable human resources and team leaders to uncover hidden patterns, identify high-potential individuals, predict future performance, assess skill gaps, and inform training needs, thereby fueling data-driven decision-making. By leveraging a knowledge graph, companies can streamline talent management and development processes, enhancing overall organizational effectiveness and fostering a culture of continuous learning and improvement.</p> <p>Incorporating a chatbot interface into this knowledge graph-driven people analytics system could revolutionize the way companies approach HR and talent management. Here\u2019s how:</p>"},{"location":"spanda-NaLLM-4/#user-friendly-access-to-complex-data","title":"User-Friendly Access to Complex Data","text":"<p>A chatbot interface provides an intuitive, conversational manner for users to interact with complex datasets. Employees, managers, or HR staff wouldn\u2019t need to understand intricate databases or analytics tools; they could simply ask the chatbot questions about employee performance, skills, or team dynamics.</p> <p>The chatbot, equipped with natural language processing capabilities, would interpret the question, retrieve the relevant information from the knowledge graph, and deliver the response in an understandable format.</p>"},{"location":"spanda-NaLLM-4/#real-time-insights","title":"Real-Time Insights","text":"<p>The chatbot interface could offer immediate access to data insights, enabling timely decisions.</p> <p>If a manager wanted to know how many projects are in the pipeline and which people are a good fit and available for a specific project, they could ask the chatbot and get an answer in real-time rather than waiting for a comprehensive report.</p>"},{"location":"spanda-NaLLM-4/#scalable-training-and-support","title":"Scalable Training and Support","text":"<p>The chatbot could provide individualized support to employees, answering questions about company policies, procedures, or career development opportunities.</p> <p>It could even deliver personalized training recommendations (and perhaps the actual training itself) based on an individual\u2019s role, skills, and career goals. This would democratize access to learning and development resources, making it easier for employees to upskill or reskill.</p>"},{"location":"spanda-NaLLM-4/#predictive-analysis","title":"Predictive Analysis","text":"<p>Advanced AI chatbots could analyze patterns and trends from the knowledge graph to make predictions, such as which employees might be at risk of leaving the company or what skills may be in demand in the future. These predictive analytics capabilities could help companies be proactive rather than reactive in their HR strategies.</p> <p>In essence, integrating a chatbot interface with a knowledge graph-driven people analytics system would make complex employee data more accessible, actionable, and useful to all members of an organization. It would be a game-changer in talent management and development, driving a more data-informed, proactive, and personalized approach to HR.</p>"},{"location":"spanda-NaLLM-4/#summary","title":"Summary","text":"<p>In conclusion, as we traverse deeper into the era of large language models, we must keep in mind the enormous potential of knowledge graphs in structuring, organizing, and retrieving information in these applications. The combination of structured and unstructured data retrieval paves the way for more accurate, reliable, and impactful results, extending beyond natural language answers into the realm of visually represented information.</p> <p>Despite the popularity of vector similarity-based data retrieval (recall), we should not underestimate the role of structured information and the immense value it brings to LLM applications. Whether it\u2019s finding the shortest paths, understanding complex biomedical relationships, analyzing supply chain scenarios, or revolutionizing HR with people analytics, the applications of knowledge graphs are vast and profound. We believe the future of LLM-based applications is the combination of vector similarity search approach coupled with database query languages such as Cypher.</p> <p>Through this blog post, we\u2019ve explored some exciting real-time graph analytics use cases that could be implemented into your LLM applications. This is only the beginning. We anticipate a future where large language models will work more cohesively with knowledge graphs, bringing about more innovative solutions to real-world problems.</p> <p>If you are new to knowledge graphs, but want to use them in your LLM-based applications, check out this free book from Jim Weber and </p> <p>Jes\u00fas Barrasa</p>"},{"location":"spanda-NaLLM-5/","title":"spanda NaLLM 5","text":"<p>&gt;&gt;&gt;&gt;&gt;  gd2md-html alert:  ERRORs: 0; WARNINGs: 1; ALERTS: 4.</p> <ul><li>See top comment block for details on ERRORs and WARNINGs. <li>In the converted Markdown or HTML, search for inline alerts that start with &gt;&gt;&gt;&gt;&gt;  gd2md-html alert:  for specific instances that need correction. <p>Links to alert messages:</p> <p>alert1 alert2 alert3 alert4</p> <p>&gt;&gt;&gt;&gt;&gt; PLEASE check and correct alert issues and delete this message and the inline alerts.</p> <p>This is the fifth blog post of Neo4j\u2019s NaLLM project. We started this project to explore, develop, and showcase practical uses of these LLMs in conjunction with Neo4j. As part of this project, we have constructed and publicly displayed demonstrations in a GitHub repository, providing an open space for our community to observe, learn, and contribute. Additionally, we have been writing about our findings in a blog post. You can read the previous four blog posts here:</p> <ul> <li>Harnessing LLMs with Neo4j</li> <li>Fine-tuning vs. Retrieval-augmented generation</li> <li>Multi-hop Question Answering</li> <li>Knowledge Graphs &amp; LLMs: Real-Time Graph Analytics</li> </ul> <p>This blog post will explore a use case we investigated during our project: extracting information from unstructured data. Organizations have long faced challenges in extracting meaningful insights from unstructured data. Such data encompasses textual content, images, audio, and other non-tabular formats, holding immense potential yet often remaining difficult to use due to its inherent complexity. Our primary focus in this post will be to extract information from unstructured text by converting it into nodes and relationships.</p> <p>&gt;&gt;&gt;&gt;&gt;  gd2md-html alert: inline image link here (to images/image1.png). Store image on your image server and adjust path/filename/extension if necessary. (Back to top)(Next alert)&gt;&gt;&gt;&gt;&gt; </p> <p></p> <p>Illustration from Imagine.art of \u201cextracting information from unstructured data\u201d</p> <p>Recent years have witnessed significant advancements in natural language processing techniques, revolutionizing the transformation of unstructured data into valuable knowledge. With the emergence of powerful language models like OpenAI\u2019s GPT models and leveraging the power of machine learning, the process of converting unstructured text data into structured representations has become more accessible and efficient.</p> <p>One such representation is knowledge graphs, which offer a robust framework for representing complex relationships and connections among various entities. They provide a structured representation of the data, enabling intuitive querying and exploration of the information contained within. This structured nature allows for advanced semantic analysis, reasoning, and inference, facilitating more accurate and comprehensive decision-making processes.</p> <p>&gt;&gt;&gt;&gt;&gt;  gd2md-html alert: inline image link here (to images/image2.png). Store image on your image server and adjust path/filename/extension if necessary. (Back to top)(Next alert)&gt;&gt;&gt;&gt;&gt; </p> <p></p> <p>Example of Knowledge Extraction Pipeline</p> <p>We will explore how Large Language Models (LLMs) have simplified the conversion of unstructured data into knowledge graphs, using an approach that utilizes the language skills of LLMs to perform nearly all parts of the process. The process can be divided into three steps:</p> <ol> <li>Extracting nodes and edges</li> <li>Entity disambiguation</li> <li>Importing into Neo4j</li> </ol> <p>Let\u2019s walk through each of these steps:</p> <p>1. Extracting nodes and relationships: To tackle this problem, we take the simplest possible approach, where we pass the input data to the LLM and let it decide which nodes and relationships to extract. We ask the LLM to return the extracted entities in a specific format, including a name, a type, and properties. This allows us to extract nodes and edges from the input text.</p> <p>However, LLMs have a limitation known as the context window (between 4 and 16,000 tokens for most LLMs), which can be easily overwhelmed by larger inputs, hindering the processing of such data. To overcome this limitation, we employ a strategy of dividing the input text into smaller, more manageable chunks that fit within the context window.</p> <p>Determining the optimal splitting points for the text is a challenge of its own. To keep things simple, we have chosen to divide the text into chunks of maximum size, maximizing the utilization of the context window per chunk. Additionally, we introduce some overlap from the previous chunk to account for cases where a sentence or description spans across multiple chunks. This approach allows us to extract nodes and edges from each chunk, representing the information contained within it.</p> <p>To maintain consistency in the labeling of different types of entities across chunks, we provide the LLM with a list of node types that were extracted in the previous chunks. Those start forming the extracted \u201cschema.\u201d We have observed that this approach enhances the uniformity of the final labels. For example, instead of the LLM generating separate types for \u201cCompany\u201d and \u201cGaming Company,\u201d it consolidates all types of companies under a \u201cCompany\u201d label.</p> <p>One notable hurdle in our approach is the problem of duplicate entities. Since each chunk is processed semi-independently, information about the same entity found in different chunks will create duplicates when we combine the results. Naturally, this issue brings us to our next step.</p> <p>2. Entity disambiguation: We now have a set of entities. To address the issue of duplication, we employ LLMs once again. First, we organize the entities into sets based on their type. Subsequently, we provide each set to the LLM, enabling it to merge duplicate entities while simultaneously consolidating their properties. We use LLMs for this since we don\u2019t know what name each entity has been given. For example, the initial extraction could have ended up with two nodes: (Alice {name: \u201cAlice Henderson\u201d}) and (Alice Henderson {age: 25}). These reference the same entity and should be merged to a single node with both the name and age property. We use LLMs to accomplish this since it\u2019s great at quickly understanding which nodes actually reference the same entity.</p> <p>By iteratively performing this procedure for all entity groups, we obtain a structured data set that is ready for further processing.</p> <p>3. Importing the data into Neo4j: In the final step of the process, we focus on importing the results we got from the LLM into a Neo4j database. This requires a format that Neo4j can understand. To accomplish this, we parse the generated text from the LLM and transform it into separate CSV files, corresponding to the various node and relationship types. These CSV files are subsequently mapped to a format compatible with the Neo4j Data Importer tool. Through this conversion, we gain the advantage of previewing the data before initiating the import process into a Neo4j database, harnessing the capabilities offered by the Neo4j Importer tool.</p> <p>&gt;&gt;&gt;&gt;&gt;  gd2md-html alert: inline image link here (to images/image3.png). Store image on your image server and adjust path/filename/extension if necessary. (Back to top)(Next alert)&gt;&gt;&gt;&gt;&gt; </p> <p></p> <p>Overview of the application</p> <p>Putting this all together, we have created an application consisting of three parts: a UI to input a file, a controller that executes the previously explained process, and an LLM that the controller talks to. This demo application can be found here, and the source code can be found on GitHub.</p> <p>We also created a version of this pipeline that works essentially in the same way but with the option to include a schema. This schema works like a filter where the user can restrict which types of nodes and relationships and which properties the LLM should include in its result.</p>"},{"location":"spanda-NaLLM-5/#nallm-graph-construction-demo","title":"NaLLM Graph Construction Demo","text":"<p>nallm-experiments.ew.r.appspot.com</p> <p>If you are interested in learning more about generative AI and knowledge graphs, I would suggest taking a look at Neo4j\u2019s page about generative AI.</p>"},{"location":"spanda-NaLLM-5/#demonstration","title":"Demonstration","text":"<p>I tested the application by giving it the Wikipedia page for the James Bond franchise and inspected the generated knowledge graph.</p> <p>&gt;&gt;&gt;&gt;&gt;  gd2md-html alert: inline image link here (to images/image4.png). Store image on your image server and adjust path/filename/extension if necessary. (Back to top)(Next alert)&gt;&gt;&gt;&gt;&gt; </p> <p></p> <p>Example of the resulting graph</p> <p>The provided graph subset showcases the generated graph, which, in my opinion, provides a reasonably accurate depiction of the Wikipedia article. The graph primarily consists of nodes representing books and individuals associated with those books, such as authors and publishers.</p> <p>However, there are a few issues with the graph. For instance, Ian Fleming is labeled as a publisher rather than an author for most of the books he wrote. This discrepancy may be attributed to the difficulty the language model had in comprehending that particular aspect of the Wikipedia article.</p> <p>Another problem is the inclusion of relationships between book nodes and the directors of films with the same titles, instead of creating separate nodes for the movies.</p> <p>Finally, It\u2019s worth noting that the LLM appears to be quite literal in its interpretation of relationships, as evidenced by using the relationship type \u201cused\u201d to connect the James Bond character with the cars he drove. This literal approach may stem from the article\u2019s usage of the verb \u201cused\u201d rather than \u201cdrove.\u201d</p> <p>A full video of the demonstration can be found here:</p> <p>Demo of KG Construction</p>"},{"location":"spanda-NaLLM-5/#problems","title":"Problems","text":"<p>For a demonstration, this approach worked fairly well, and we think it shows that it\u2019s possible to use LLMs to create knowledge graphs. However, we acknowledge certain issues need to be addressed within this approach:</p> <ul> <li>Unpredictable output: This is inherent to the nature of LLMs. We do not know how an LLM will format its results. Even if we ask it to output in a specific format, it might not obey. This might cause problems when trying to parse what it generates. We saw one instance of this while chunking the data: Most of the time, the LLM generated a simple list of nodes and edges, but sometimes the LLM would number the list. Tools to work around this are starting to be released, such as Guardrails and OpenAIs Function API. It\u2019s still early in the world of LLM tooling, so we anticipate that this will not be a problem for long.</li> <li>Speed: This approach is slow and often takes several minutes for just a single reasonably large web page. There might be a fundamentally different approach that can make the extraction go faster.</li> <li>Lack of accountability: There is no way of knowing why the LLM decided to extract some information from the source documents or if the information even exists in the source. The data quality of the resulting knowledge graph is, therefore, much lower than the graph created by processes not leveraging LLMs.</li> </ul>"},{"location":"spanda-NaLLM-5/#summary","title":"Summary","text":"<p>This blog post explored a use case of Large Language Models with Neo4j to extract insights from unstructured data by converting it into a structured representation in the form of a knowledge graph.</p> <p>We discussed a three-step approach focusing on extracting nodes and relationships, entity disambiguation, and importing the data into Neo4j. By utilizing LLMs, anyone can automate the extraction process and efficiently process large amounts of unstructured data.</p> <p>However, there are challenges to address, including unpredictable output formatting, speed limitations, and the lack of accountability. Despite these issues, the combined power of LLMs and Neo4j offers a promising solution for unlocking the hidden value in unstructured data, even for non-technical users.</p> <p>We hope that this blog post has provided you with valuable insights and practical knowledge to leverage LLMs and Neo4j in extracting knowledge from unstructured data. The code for this project can be found on GitHub, and if you want to know more about AI and Neo4j, I suggest taking a look at Neo4j\u2019s page about generative AI.</p>"}]}